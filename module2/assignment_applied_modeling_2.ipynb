{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_applied_modeling_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nCc3XZEyG3XV"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 3, Module 2*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Permutation & Boosting\n",
        "\n",
        "You will use your portfolio project dataset for all assignments this sprint.\n",
        "\n",
        "## Assignment\n",
        "\n",
        "Complete these tasks for your project, and document your work.\n",
        "\n",
        "- [X] Plot the distribution of your target. \n",
        "    - Classification problem: Are your classes imbalanced? Then, don't use just accuracy.\n",
        "    - Regression problem: Is your target skewed? If so, let's discuss in Slack.\n",
        "- [X] Continue to clean and explore your data. Make exploratory visualizations.\n",
        "- [ ] Fit a model. Does it beat your baseline?\n",
        "- [ ] Try xgboost.\n",
        "- [ ] Get your model's permutation importances.\n",
        "\n",
        "You should try to complete an initial model today, because the rest of the week, we're making model interpretation visualizations.\n",
        "\n",
        "\n",
        "## Reading\n",
        "\n",
        "Top recommendations in _**bold italic:**_\n",
        "\n",
        "#### Permutation Importances\n",
        "- _**[Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)**_\n",
        "- [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
        "\n",
        "#### (Default) Feature Importances\n",
        "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
        "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
        "\n",
        "#### Gradient Boosting\n",
        "  - [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
        "  - _**[A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)**_\n",
        "  - [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) Chapter 8\n",
        "  - [Gradient Boosting Explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)\n",
        "  - _**[Boosting](https://www.youtube.com/watch?v=GM3CDQfQ4sw) (2.5 minute video)**_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bof7jQoOHomM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "774b4e73-0370-4e78-fbc1-c1852e7ed0a9"
      },
      "source": [
        "!pip install category_encoders==2.*"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders==2.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.*) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.*) (1.16.5)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.*) (0.24.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.*) (1.3.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.*) (0.10.1)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.*) (0.5.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders==2.*) (0.14.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders==2.*) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders==2.*) (2.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders==2.*) (1.12.0)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuJXgP4fQZi7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "c694614a-503f-4b0c-c5c3-768603b7da00"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/apathyhill/DS-Unit-2-Applied-Modeling/master/data/portfolio/vgsales-12-4-2019.csv\")\n",
        "data2 = data.copy()\n",
        "#data2[\"score\"] = data2[[\"VGChartz_Score\", \"Critic_Score\", \"User_Score\"]].mean(axis=1)\n",
        "data2 = pd.concat([data2, pd.get_dummies(data2[\"ESRB_Rating\"])], axis=1)\n",
        "\n",
        "features_excluded = [\"Last_Update\", \"url\", \"status\", \"Vgchartzscore\", \"img_url\", \"basename\", \n",
        "                     \"NA_Sales\", \"PAL_Sales\", \"JP_Sales\", \"Other_Sales\", \"Rank\", \"Name\", \n",
        "                     \"VGChartz_Score\", \"Critic_Score\", \"User_Score\", \"ESRB_Rating\"]\n",
        "\n",
        "data2 = data2.drop(features_excluded, axis=1)\n",
        "\n",
        "data2[\"Year\"] = data2[\"Year\"].fillna(data2[\"Year\"].median())\n",
        "data2[\"Total_Shipped\"] = data2[\"Total_Shipped\"].fillna(method=\"ffill\")\n",
        "data2[\"Global_Sales\"] = data2[\"Global_Sales\"].fillna(method=\"ffill\")\n",
        "data2 = data2.dropna()\n",
        "\n",
        "print(data2.isna().sum())\n",
        "print(\"Columns:\", list(data2.columns))\n",
        "target = \"Global_Sales\"\n",
        "\n",
        "train, test = train_test_split(data2, train_size=0.90)\n",
        "train, val = train_test_split(train, train_size=0.75)\n",
        "\n",
        "\"\"\"\n",
        "This is a regression problem, as I am trying to output a continous number.\n",
        "\n",
        "Simple accuracy could work as a eval matric right now, but I might change it to mean absolue/squared error to see if I can decipher things better.\n",
        "\"\"\"\n",
        "\n",
        "features_excluded = [\"Last_Update\", \"url\", \"status\", \"Vgchartzscore\", \"img_url\", \"basename\", \n",
        "                     \"NA_Sales\", \"PAL_Sales\", \"JP_Sales\", \"Other_Sales\", \"Rank\", \"Name\", \n",
        "                     \"VGChartz_Score\", \"Critic_Score\", \"User_Score\", \"ESRB_Rating\"]\n",
        "\"\"\"\n",
        "These last columns are only for information about the game on the VGChartz website, such as cover images, and the url to the game. Not important here.\n",
        "\"basename\" is a web-safe, simplified name of the game. (Wii Sports becomes wii-sports)\n",
        "\"NA_Sales\", \"PAL_Sales\", \"JP_Sales\", \"Other_Sales\" are just  \"Global_Sales\" split into regions; probably not necessary.\n",
        "\"Rank\" is not needed, as it is functionally just an index.\n",
        "Each entry is a new game with its own title, so the \"Name\" category is *very* high cardinality, too much to use.\n",
        "\n",
        "\n",
        "This leaves Genre, ESRB Rating (in seperate columns), Platform, Plublisher, Developer, and Year\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#data2[target].plot(kind=\"hist\", bins=50, figsize=(20, 10));\n",
        "\n",
        "mean = data2[target].mean()\n",
        "print(\"Baseline Accuracy:\", len(data2[data2[target] < mean])/ len(data2[target]) ) # Baseline is 178,000 sales for 82% accuracy\n",
        "\n",
        "\n",
        "X_train = train[train.columns.drop(target)]\n",
        "y_train = train[target]\n",
        "\n",
        "X_val = val[val.columns.drop(target)]\n",
        "y_val = val[target]\n",
        "\n",
        "X_test = test[val.columns.drop(target)]\n",
        "y_test = test[target]\n",
        "\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import category_encoders as ce\n",
        "import numpy as np\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy=\"median\", missing_values=np.nan), \n",
        "    Ridge()\n",
        ")\n",
        "\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(\"Validation Accuracy:\", pipeline.score(X_val, y_val))\n",
        "print(\"Test Accuracy:\", pipeline.score(X_test, y_test))\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Genre            0\n",
            "Platform         0\n",
            "Publisher        0\n",
            "Developer        0\n",
            "Total_Shipped    0\n",
            "Global_Sales     0\n",
            "Year             0\n",
            "AO               0\n",
            "E                0\n",
            "E10              0\n",
            "EC               0\n",
            "KA               0\n",
            "M                0\n",
            "RP               0\n",
            "T                0\n",
            "dtype: int64\n",
            "Columns: ['Genre', 'Platform', 'Publisher', 'Developer', 'Total_Shipped', 'Global_Sales', 'Year', 'AO', 'E', 'E10', 'EC', 'KA', 'M', 'RP', 'T']\n",
            "Baseline Accuracy: 0.8299734557715761\n",
            "Validation Accuracy: 0.9962755163019107\n",
            "Test Accuracy: 0.9991348142347253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IrgsE5DK-ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}