{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of assignment_applied_modeling_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dunkelweizen/DS-Unit-2-Applied-Modeling/blob/master/Cai%20Nowicki%20assignment_applied_modeling_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nCc3XZEyG3XV"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 3, Module 2*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Permutation & Boosting\n",
        "\n",
        "You will use your portfolio project dataset for all assignments this sprint.\n",
        "\n",
        "## Assignment\n",
        "\n",
        "Complete these tasks for your project, and document your work.\n",
        "\n",
        "- [ ] If you haven't completed assignment #1, please do so first.\n",
        "- [ ] Continue to clean and explore your data. Make exploratory visualizations.\n",
        "- [ ] Fit a model. Does it beat your baseline? \n",
        "- [ ] Try xgboost.\n",
        "- [ ] Get your model's permutation importances.\n",
        "\n",
        "I had to run my model locally because it was taking too long in Colab. My initial models were giving me a validation accuracy of around 21-22% which isn't great but is better than baseline. With XGBoost, I got a validation accuracy of around 31%. \n",
        "Since I'm using leave-one-out-cross-validation instead of a train/test/validation split, I had to fit permutation importance to my training set, which shows how important different features are to that set but does not generalize to other sets of data. \n",
        "\n",
        "You should try to complete an initial model today, because the rest of the week, we're making model interpretation visualizations.\n",
        "\n",
        "But, if you aren't ready to try xgboost and permutation importances with your dataset today, that's okay. You can practice with another dataset instead. You may choose any dataset you've worked with previously.\n",
        "\n",
        "The data subdirectory includes the Titanic dataset for classification and the NYC apartments dataset for regression. You may want to choose one of these datasets, because example solutions will be available for each.\n",
        "\n",
        "\n",
        "## Reading\n",
        "\n",
        "Top recommendations in _**bold italic:**_\n",
        "\n",
        "#### Permutation Importances\n",
        "- _**[Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)**_\n",
        "- [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
        "\n",
        "#### (Default) Feature Importances\n",
        "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
        "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
        "\n",
        "#### Gradient Boosting\n",
        "  - [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
        "  - _**[A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)**_\n",
        "  - [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) Chapter 8\n",
        "  - [Gradient Boosting Explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)\n",
        "  - _**[Boosting](https://www.youtube.com/watch?v=GM3CDQfQ4sw) (2.5 minute video)**_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXq86vrmZebW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Dec  4 21:11:16 2019\n",
        "\n",
        "@author: caino\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "#%%explore and clean data\n",
        "\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/thanksgiving-2015/thanksgiving-2015-poll-data.csv')\n",
        "\n",
        "\n",
        "\n",
        "df.columns = ['ID', 'Celebrate', 'Main_Dish', 'Main_Dish_Other', 'Main_Dish_Cooked', 'Main_Dish_Cooked_Other', 'Stuffing',\n",
        "              'Stuffing_Other', 'Cranberry_Sauce', 'Cranberry_Sauce_Other', 'Gravy', 'Brussel_Sprouts', 'Carrots', 'Cauliflower',\n",
        "              'Corn', 'Cornbread', 'Fruit_Salad', 'Green_Beans', 'Mac_and_Cheese', 'Mashed_Potatoes', 'Rolls_Biscuits', 'Squash',\n",
        "              'Salad', 'Sweet_Potatoes', 'Side_Dish_Other1', 'Side_Dish_Other2', 'Apple_Pie', 'Buttermilk_Pie', 'Cherry_Pie', \n",
        "              'Chocolate_Pie', 'Coconut_Cream_Pie', 'Key_Lime_Pie', 'Peach_Pie', 'Pecan_Pie', 'Pumpkin_Pie', 'Sweet_Potato_Pie', \n",
        "              'No_Pie', 'Other_Pie1', 'Other_Pie2', 'Apple_Cobbler', 'Blondies', 'Brownies', 'Carrot_Cake', 'Cheesecake', 'Cookies',\n",
        "              'Fudge', 'Ice_Cream', 'Peach_Cobbler', 'No_Dessert', 'Other_Dessert1', 'Other_Dessert2', 'Prayer', 'Travel_Distance', 'Parade',\n",
        "              'Kids_Table_Age', 'Old_Friends', 'Friendsgiving', 'Black_Friday_Shopper', 'Retail_Worker', 'Black_Friday_Worker', 'Neighborhood_Type',\n",
        "              'Age', 'Gender', 'Household_Earnings', 'US_Region']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "side_dishes = ['Stuffing',\n",
        "              'Stuffing_Other', 'Cranberry_Sauce', 'Cranberry_Sauce_Other', 'Gravy', 'Brussel_Sprouts', 'Carrots', 'Cauliflower',\n",
        "              'Corn', 'Cornbread', 'Fruit_Salad', 'Green_Beans', 'Mac_and_Cheese', 'Mashed_Potatoes', 'Rolls_Biscuits', 'Squash',\n",
        "              'Salad', 'Sweet_Potatoes', 'Side_Dish_Other1', 'Side_Dish_Other2', 'Apple_Pie', 'Buttermilk_Pie', 'Cherry_Pie', \n",
        "              'Chocolate_Pie', 'Coconut_Cream_Pie', 'Key_Lime_Pie', 'Peach_Pie', 'Pecan_Pie', 'Pumpkin_Pie', 'Sweet_Potato_Pie', \n",
        "              'No_Pie', 'Other_Pie1', 'Other_Pie2', 'Apple_Cobbler', 'Blondies', 'Brownies', 'Carrot_Cake', 'Cheesecake', 'Cookies',\n",
        "              'Fudge', 'Ice_Cream', 'Peach_Cobbler', 'No_Dessert', 'Other_Dessert1', 'Other_Dessert2']\n",
        "#get rid of any rows where they don't celebrate\n",
        "df = df.drop(df[df.Celebrate == 'No'].index)\n",
        "df = df.drop('Celebrate', axis = 1)\n",
        "#remove any rows where no US_Region is given\n",
        "df = df.drop(df[df.US_Region == np.NaN].index)\n",
        "\n",
        "\n",
        "\n",
        "#%%\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmw3v4-KdcqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#%% create data wrangling function\n",
        "\n",
        "def wrangle(df):\n",
        "    #data is formatted so that a column has the name of the dish if true and NaN if false\n",
        "#so replace all the NaN with False and all values with True\n",
        "#also for Parade column\n",
        "    for column in df.columns:\n",
        "        if column in side_dishes:\n",
        "            df[column] = df[column].fillna('No')\n",
        "        elif column == 'Parade':\n",
        "            df[column] = df[column].fillna('No')\n",
        "#removing the 'Other' columns for simplicities' sake\n",
        "    df = df.replace('nan', np.NaN)\n",
        "    for column in df.columns:\n",
        "        if 'Other' in column:\n",
        "            df = df.drop(column, axis=1)\n",
        "    df = df.drop('ID', axis=1)\n",
        "#replace empty values in Black_Friday_Worker with 'No' if person isn't in retail      \n",
        "    for i in range(len(df)):\n",
        "        if df['Retail_Worker'].iloc[i] == 'No':\n",
        "            df['Black_Friday_Worker'].iloc[i] = 'No'\n",
        "#fix NaN values in other columns\n",
        "    df['Kids_Table_Age'] = df['Kids_Table_Age'].fillna('No Kids Table')\n",
        "    mode = df['Household_Earnings'].mode()[0]\n",
        "    df['Household_Earnings'] = df['Household_Earnings'].fillna(mode)\n",
        "    mode = df['Age'].mode()[0]\n",
        "    df['Age'] = df['Age'].fillna(mode)\n",
        "    mode = df['Travel_Distance'].mode()[0]\n",
        "    df['Travel_Distance'] = df['Travel_Distance'].fillna(mode)\n",
        "    df['Neighborhood_Type'] = df['Neighborhood_Type'].fillna('Other')\n",
        "    df['Main_Dish'] = df['Main_Dish'].fillna(\"I don't know\")\n",
        "    df['Main_Dish_Cooked'] = df['Main_Dish_Cooked'].fillna(\"I don't know\")\n",
        "    df = df.fillna('No')\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJaEPsnSdhpY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "68df5f56-0e64-48a1-fc3e-bb4948e78042"
      },
      "source": [
        "#%%splitting data\n",
        "\n",
        "train, test = train_test_split(df)\n",
        "train = wrangle(train)\n",
        "test = wrangle(test)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZTd-DBJdmfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#%% creating X and y\n",
        "def X_y_dataframes(train, test, target):\n",
        "    X_train = train.drop(target, axis=1)\n",
        "    X_test = test.drop(target, axis=1)\n",
        "    y_train = train[target]\n",
        "    y_test = test[target]\n",
        "    #ordinal encoding\n",
        "    encoder = OrdinalEncoder()\n",
        "    X_train_encoded = encoder.fit_transform(X_train)\n",
        "    X_test_encoded = encoder.fit_transform(X_test)\n",
        "    #converting encoded arrays back to dataframes\n",
        "    X_train_encoded = pd.DataFrame(X_train_encoded)\n",
        "    X_train_encoded.columns = X_train.columns\n",
        "    X_test_encoded = pd.DataFrame(X_test_encoded)\n",
        "    X_test_encoded.columns = X_test.columns\n",
        "    return X_train_encoded, X_test_encoded, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSSmuEZpdiNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#%% leave one out validation\n",
        "def loocv(X_train_encoded, y_train, model):\n",
        "    num_folds = 10\n",
        "    num_instances = len(X_train_encoded)\n",
        "    loocv = LeaveOneOut()\n",
        "    results = cross_val_score(model, X_train_encoded, y_train, cv=loocv)\n",
        "    print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO6tqrkYdqDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a58969a5-ffc4-4c29-c1d9-38fcea699cda"
      },
      "source": [
        "\n",
        "#%%\n",
        "target = 'US_Region'\n",
        "X_train_encoded, X_test_encoded, y_train, y_test = X_y_dataframes(train, test, target)\n",
        "#%%Baseline model\n",
        "print(y_train.value_counts(normalize=True))\n",
        "baseline = 'South Atlantic'"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "South Atlantic        0.208163\n",
            "East North Central    0.146939\n",
            "Pacific               0.140136\n",
            "Middle Atlantic       0.140136\n",
            "West South Central    0.091156\n",
            "West North Central    0.076190\n",
            "New England           0.055782\n",
            "East South Central    0.053061\n",
            "No                    0.048980\n",
            "Mountain              0.039456\n",
            "Name: US_Region, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvPz-ZwffLA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "6d88137e-a13d-4855-f0cb-4909e17b9743"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x=X_train_encoded['Main_Dish'], y=y_train)\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcQAAAD4CAYAAABykJZ9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3df5yVdZ338de7kWLEH3OX6C0TNK0a\npo6Cjoma3lgStVlN5N5KshtbK7e7m6SutFA8LAvDpDTNftyYZoXiZtKsaQt6J6ysCjE4yqhJZiE4\ntInZpOiIePrcf5zv4GE4Z+ac8QxnzvB+Ph7z8Fyf67q+1+dcnsd8zvd7fYevIgIzM7M93RsqnYCZ\nmdlg4IJoZmaGC6KZmRnggmhmZga4IJqZmQGwV6UTsP474IADoqGhodJpmJlVlbVr1z4bESN7xl0Q\nq1hDQwOtra2VTsPMrKpIeipf3EOmZmZmuCCamZkBLohmZmaAC6KZmRnggmhmZgb0oyBKykh6KOdn\ndj/amCjppAL7DpJ0h6SHJT0m6eeltp/T1gWS9s7Z3lrkeR+Q1Jqu3ybp6/28fp2kf+rnuRskHdCf\nc83MrHT9+bOLrogY9zqvOxHYCtyfZ9+XgLsj4moASUe/jutcACwCXir2BElHAdcCH4yIxyXVADP6\nef064J+Ab+e5zl4R8Wo/291jtLR1sGDZejZ3djGqrpZZk8fSPL6+0mkNCZOuXMETz7y4Y/uwA0dw\n90UTK5dQL/w5MICG2XfuEttw+QfL1n7ZhkwlXSJpjaRHJC2UpBSfmXpa6yTdIqkBOA+4MPUwT+nR\n1MHA090bEbEutSNJC1L77ZLOSvGJku7IyeNaSdMlzQRGAcslLc/Zf1nqfa6SdFCet/JZ4LKIeDxd\nPxMR30nnjpR0W3qfaySdnOJflHSDpBWSfpuuDXA5cEh6nwtSrisl3Q48ls5tkbRW0qOS+lt4h6SW\ntg7mLGmno7OLADo6u5izpJ2Wto5Kp1b1ehZDgCeeeZFJV66oTEK98OfAIH8x7C3eH/0piLU9hkzP\nSvFrI+L4iDgKqAXOSPHZwPiIOBo4LyI2AN8FroqIcRGxskf73wKul7Rc0ucljUrxKcA44BjgdGCB\npIMLJRkR1wCbgdMi4rQUHgGsiohjgHuBc/OcehSwtkCzV6e8jwc+BnwvZ9/hwGTgXcAXJA1L7/3J\n9D5npeOOBT4TEe9I25+MiOOAJmCmpLcUek97mgXL1tO1PbNTrGt7hgXL1lcoo6GjZzHsK15J/hzY\n7lLOIdPTJH0W2Bt4M/Ao8DNgHXCTpBagpa/GI2KZpL8C3g98AGhLw5jvBhZHRAb4g6T/BI4Hni8h\n91eA7t7kWmBSCedCthAfkTq/APtJ2ie9vjMitgHbJD0D5Ot9AvwyIn6Xsz1T0kfT69HAYcAfCyWQ\nepEzAMaMGVNi+tVlc2dXSXEbmvw5sN2lLEOmkoaTfU52ZkQ0AtcBw9PuD5Lt9R0LrJHUZxGOiOci\n4uaI+FtgDXBqL4e/ys7vY3ihA4HtERHpdYb8XwgeBY4rcP4bgAmpxzcuIuojonuizrac4wq1DbDj\nK7ikiWSL7Imp19rWR/5ExMKIaIqIppEjd/mn+IaUUXW1JcVtaPLnwHaXcj1D7P4l/mzqMZ0JIOkN\nwOiIWA78K7A/sA/wArBvvoYkvad7ZqikfYFDgI3ASuAsSTWSRpItkr8EniLba3uTpDrgvTnNFbxO\nLxYAn5P0ju73IOm8tO8u4PycXPuaXNTX9fcH/hQRL0k6HJhQYq5D2qzJY6kdVrNTrHZYDbMmj61Q\nRkPHYQeOKCleSf4c2O5SjmeIl0dEJ9le4SPAMrK9OoAaYJGkdrK9n2vSsT8DPlpgUs1xQKukdcAD\nwPciYg3wU7LDrw8D9wCfjYj/johNwI/TtX+crtNtIbA0d1JNX9IknguAxZJ+ldr9q7R7JtCUJgg9\nRnZyUG9t/RG4L00EWpDnkKXAXuk6lwOris1zT9A8vp75Uxqpr6tFQH1dLfOnNHp2YRncfdHEXYrf\nYJ1l6s+BQeHZpOWcZarXRhCt2jQ1NYVXuzAzK42ktRHR1DPuf6nGzMwMF0QzMzPABdHMzAxwQTQz\nMwNcEM3MzAAXRDMzM8AF0czMDHBBNDMzA1wQzczMABdEMzMzwAXRzMwMcEE0MzMDXBDNzMyAwovY\nAiDpKuCpiPhG2l4GbIqIf0jbXwc6IuLKUi4q6QJgYUS8lGffGcCXyRbrYcDVEfF/S2k/tdMAnBQR\nN6ft6UBTRHy6j/OGpet/jOx6htuAL0XEf/Qjh4nAKxFxfz/Ouzgizij1mkNNS1sHC5atZ3NnF6Pq\napk1eayX/SkT39uBMbelncWrN5GJoEZi6gmjmdfcWOm0hoSG2XfuEivn8k999RDvA06CHYv9HgAc\nmbP/JKCkX/bJBcDePYOpGC0EPpRWkB8PrOhH+wANwMf7cd6XgYOBoyLiWKCZ0hcZ7jaRdP96ktTr\nlxHL/sKes6Sdjs4uAujo7GLOknZa2joqnVrV870dGHNb2lm0aiOZtKxeJoJFqzYyt6W9wplVv3zF\nsLd4f/RVEO8HTkyvjyS7WO4Lkv6HpDcB7wQeBJA0S9KatHjupSk2QtKdkh5Oi+SeJWkmMApYnmfh\n3n3J9lr/CBAR2yJifWqrQdI9qf1fSBqT4jdKOrO7AUlb08vLgVPSIsQXptgoSUslPSHpip5vVtLe\nwLnA+RGxLeXwh4j4cdr/PkkPSHpQ0q2S9knxDZIuTfF2SYenHup5wIXdCyGnXL8raTVwhaR3pfba\nJN0vyUuA51iwbD1d2zM7xbq2Z1iwbH2FMho6fG8HxuLVm0qK2+DSa0GMiM3Aq6n4nER2BfvVZItk\nE9AeEa9Ieh9wGPAuYBxwnKRTgfcDmyPimIg4ClgaEdcAm4HTIuK0Htd7DrgdeErSYknnpJ4pwDeB\nH0TE0cBNwDV9vLfZwMqIGBcRV6XYOOAsoBE4S9LoHuccCmyMiOd7NibpAGAucHrqObYCF+Uc8myK\nf4fscOcG4LvAVSmHlem4t5Idyr0IeBw4JSLGA5cAX+njPSFphqRWSa1btmzp6/Cqtrmzq6S4Fc/3\ndmBkCiy4Xihug0sxk2ruJ1sMuwviAznb96Vj3pd+2sj2GA8nWyDbgUmSvirplIj4c18XS88n3wv8\nErgYuCHtOhG4Ob3+EfDuInLv6RcR8eeIeBl4DHhbCedOAI4A7pP0EPCJHucvSf9dS3a4tpBbI6L7\nq/n+wK2SHgGuYufh6LwiYmFENEVE08iRI0tIv/qMqqstKW7F870dGDVSSXEbXIopiN3PERvJDpmu\nIluccp8fCpifekLjIuLQiLg+In4NHEu2MM6TdEkxSUVEe+rVTSI7uaU3r3a/j9SbfGMvx27LeZ1h\n10lFvwHGSNovz7kC7s55j0dExKfytJ2v3Vwv5rz+MrA89Z4/BAzv5bw9zqzJY6kdVrNTrHZYDbMm\ne2T59fK9HRhTT+g56NR73AaXYnuIZwDPRUQmDWvWkS2K3QVxGfDJnGdq9ZIOlDQKeCkiFgELyBZH\nyM7e3GWiiqR90gzLbuOAp3LyODu9PgfoHoLcAByXXn+Y7MzUgtfoTZr1ej1wtaQ3ppxGSvobsl8E\nTpZ0aIqPkPSOPprsK4f9ge5ZDNNLyXVP0Dy+nvlTGqmvq0VAfV0t86c0eiZkGfjeDox5zY1MmzBm\nR4+wRmLahDGeZVoGhWaTlnOWaTEzHdvJzi69uUdsn4h4FiAi7pL0TuABZT8IW4FpZJ/JLZD0F2A7\n8I/p/IXAUkmbezxHFPBZSf8X6CLbm5qe9p0PfF/SLGAL8Pcpfh3w75IeBpbyWg9sHZBJ8RuBPxXx\nXiH7nHAe8Jikl1N7l0TElvSnG4vThKLuY3/dS1s/A34i6SMp/56uAH4gaS5QvqlSQ0jz+Hr/kh4g\nvrcDY15zowvgACln8ctH4Ye9VaupqSlaW1srnYaZWVWRtDYimnrG/S/VmJmZ4YJoZmYGuCCamZkB\nLohmZmaAC6KZmRnggmhmZga4IJqZmQEuiGZmZoALopmZGeCCaGZmBrggmpmZAS6IZmZmgAuimZkZ\nUNzyT0OCpABuiohpaXsv4PfA6og4o8zXmg7cFRGb+zjuS8C9EfH/ynn93rS0dbBg2Xo2d3Yxqq6W\nWZPHDuolgKotX7O5Le0sXr2JTAQ1ElNPGO3loMqkYfauq+SVc0moPamH+CJwlKTatD2J1xbnLbfp\nwKi+DoqIS3Z3MZyzpJ2Ozi4C6OjsYs6SdlraBuo2vD7Vlq/Z3JZ2Fq3aSCYtq5eJYNGqjcxtaa9w\nZtUvXzHsLd4fe1JBBPg50P11YiqwuHuHpDdLapG0TtIqSUen+BclXZxz3COSGtLPryRdJ+lRSXdJ\nqpV0JtAE3CTpoRS7RNKadO5CpVWUJd2YjkfSBkmXSnpQUrukw8v95hcsW0/X9sxOsa7tGRYsW1/u\nS5VFteVrtnj1ppLiNrjsaQXxFuBsScOBo4HVOfsuBdoi4mjgc8APi2jvMOBbEXEk0Al8LCJ+ArQC\n50TEuIjoAq6NiOMj4iigFig0RPtsRBwLfAe4ON8BkmZIapXUumXLliJSfM3mzq6S4pVWbfmaZQos\nuF4oboPLHlUQI2Id0EC2d/jzHrvfDfwoHXcP8BZJ+/XR5O8i4qH0em1qO5/TJK2W1A68BziywHFL\n+morIhZGRFNENI0cObKP9HY2qq62pHilVVu+ZjXZwZ+i4za47FEFMbkd+Bo5w6V9eJWd79PwnNfb\ncl5nyDNJKfVGvw2cGRGNwHU92sjV3V7etl6vWZPHUjusZqdY7bAaZk0eW+5LlUW15Ws29YTRJcVt\ncNkTC+INwKUR0fMp90rgHABJE8kOXz4PbACOTfFjgbcXcY0XgH3T6+7i96ykfYAzX0/yr0fz+Hrm\nT2mkvq4WAfV1tcyf0jhoZ21WW75m85obmTZhzI4eYY3EtAljPMu0DArNJi3nLNM95s8uukXE08A1\neXZ9EbhB0jrgJeATKX4b8HeSHiX7zPHXRVzmRuC7krqAE8n2Ch8B/htY83ryf72ax9dXVUGptnzN\n5jU3ugAOkHIWv3wUfthbtZqamqK1tbXSaZiZVRVJayOiqWd8TxwyNTMz24ULopmZGS6IZmZmgAui\nmZkZ4IJoZmYGuCCamZkBLohmZmaAC6KZmRnggmhmZga4IJqZmQEuiGZmZoALopmZGeCCaGZmBvRz\n+SdJGSB3PcFbIuLyEtuYCLwSEffn2Ted7LqF49Iq90h6BDgjIjaUcI0LgIUR8VLa3hoR+xRx3geA\nLwN7k120956I+Jdir5vTTh3w8Yj4dj/O3QA0RcSzpZ47lLS0dbBg2Xo2d3Yxqq6WWZPHejmoMpl0\n5QqeeObFHduHHTiCuy+aWLmEhgh/ZgdOw+w7d4mVc0mo/vYQuyJiXM5PScUwmQic1Mv+p4HP9ys7\nQFINcAHZolbKeUcB1wLTIuIIoAn4TT/TqAP+qcB19ri1KEvV0tbBnCXtdHR2EUBHZxdzlrTT0tZR\n6dSqXs9iCPDEMy8y6coVlUloiPBnduDkK4a9xfujrEOmki6RtEbSI5IWStlloyXNlPSYpHWSbpHU\nAJwHXCjpIUmn5GnuDuBISWPzXGeqpPZ0na/mxLdK+rqkh8kW01HAcknLc465TNLDklZJOijPdT8L\nXBYRjwNERCYivpPOHSnptvQe10g6OcW/KOkGSSsk/VbSzNTW5cAh6T0ukDRR0kpJtwOPpXNbJK2V\n9KikGaXc76FuwbL1dG3P7BTr2p5hwbL1Fcpo6OhZDPuKW3H8ma1u/S2ItemXfPfPWSl+bUQcHxFH\nAbXAGSk+GxgfEUcD56Vhz+8CV6Ue5so81/gLcAXwudygpFHAV4H3AOOA4yU1p90jgNURcUxEfAnY\nDJwWEafl7F8VEccA9wLn5rnuUcDaAu/76pTz8cDHgO/l7DscmAy8C/iCpGHpfT+Z3uOsdNyxwGci\n4h1p+5MRcRzZnuhMSW8pcO3u9z9DUquk1i1btvR2aNXb3NlVUtys0vyZrW7lGjL9txQ/TdJqSe1k\nC9aRKb4OuEnSNODVEq5zMzBB0ttzYscDKyJiS0S8CtwEnJr2ZYDbemnvFbI9T8gWvYYScgE4HbhW\n0kPA7cB+krqfSd4ZEdvSM79ngHy9T4BfRsTvcrZnph7tKmA0cFhvCUTEwohoioimkSNHlph+dRlV\nV1tS3KzS/JmtbmUbMpU0HPg2cGZENALXAcPT7g8C3yLbO1pT7POzVPC+DvxrkWm8HBGZXvZvj4hI\nrzPkn1T0KHBcgfPfAEzI+SJQHxFb075tOccVahtgx5hUmlh0OnBi6rW28do92+PNmjyW2mE1O8Vq\nh9Uwa/Iuo+hWosMOHFFS3Irjz2x1K+czxO5f5M+mXtOZAJLeAIyOiOVkC9v+wD7AC8C+RbR7I9mi\n0d0d+iXwvyQdkCbOTAX+s8C5xV4j1wLgc5Le0Z2/pPPSvruA87sPlDSuj7b6uv7+wJ8i4iVJhwMT\nSsx1SGseX8/8KY3U19UioL6ulvlTGj1jrwzuvmjiLsXPs0xfP39mB06h2aTlnGXa35mOtWnYsNvS\niJgt6TrgEeC/gTVpXw2wSNL+gIBrIqJT0s+An0j6CHB+geeIRMQrkq4h+/yOiPi9pNnA8tTenRHx\n7wXyXAgslbQ55zliryJiXfpzjcWS9gaC14ZZZwLfkrSO7L27l+zkoEJt/VHSfelPRv4D6Dkdailw\nnqRfAevJDptajubx9f5lMkBc/AaGP7MDp5zFLx+9NoJo1aapqSlaW1srnYaZWVWRtDYimnrG/S/V\nmJmZ4YJoZmYGuCCamZkBLohmZmaAC6KZmRnggmhmZga4IJqZmQEuiGZmZoALopmZGeCCaGZmBrgg\nmpmZAS6IZmZmgAuimZkZ0P/ln6wfJAVwZUT8S9q+GNgnIr64u3KY29LO4tWbyERQIzH1hNHMa27c\nXZcvWTXle/jnf87LmddWjxleIx6/7K8rmNHQUU2fAxs4DbN7rqBX3iWh3EPcvbYBUyQdUImLz21p\nZ9GqjWTSkl+ZCBat2sjclvZKpNOnasq3ZzEEeDkTHP75n1coo6Gjmj4HNnDyFcPe4v3hgrh7vUp2\n0eILe+6Q1CDpHknrJP1C0phyX3zx6k0lxSutmvLtWQz7ilvxqulzYNXNBXH3+xZwjqT9e8S/Cfwg\nIo4GbgKuyXeypBmSWiW1btmypaQLZwosBl0oXmnVlq8NDH8ObHdxQdzNIuJ54IfAzB67TgRuTq9/\nBLy7wPkLI6IpIppGjhxZ0rVrpJLilVZt+drA8OfAdhcXxMr4BvApYMTuvOjUE0aXFK+0asp3eE3+\nX86F4la8avocWHVzQayAiHgO+DHZotjtfuDs9PocYGW5rzuvuZFpE8bs+GZdIzFtwphBO1uvmvJ9\n/LK/3qX4eZZpeVTT58AGTqHZpOWcZarwOPxuI2lrROyTXh8E/A64IiK+KOltwPeBA4AtwN9HxMbe\n2mtqaorW1taBTtvMbEiRtDYimnrG/XeIu1F3MUyv/wDsnbP9FPCeSuRlZmYeMjUzMwNcEM3MzAAX\nRDMzM8AF0czMDHBBNDMzA1wQzczMABdEMzMzwAXRzMwMcEE0MzMDXBDNzMwAF0QzMzPABdHMzAxw\nQTQzMwMG0WoXkj4PfBzIAH8B/k9ErO5HOxOBVyLi/rR9I3BHRPykiHObgZ8C74yIx1OsATgpIm7O\naf/iiDij1NzS+Z+LiK/kbN8fESf1p609wdFfWMrz2zI7tvd7Uw3rLn1/BTMaOs657gHue/K5Hdsn\nH/Jmbjr3xApmNDTMbWln8epNZCKokZh6wmiv3VgmDbPv3CVWzvUQB0UPUdKJwBnAsRFxNHA6sKmf\nzU0E+ltgpgL/lf7brYFsoS6Xz+VuuBgW1rMYAjy/LcPRX1haoYyGjp7FEOC+J5/jnOseqFBGQ8Pc\nlnYWrdpIJq0zm4lg0aqNzG1pr3Bm1S9fMewt3h+DoiACBwPPRsQ2gIh4NiI2A0h6r6Q2Se2SbpD0\nphTfIOmA9LpJ0orUmzsPuFDSQ5JOSe2fKul+Sb+VdGa+BCTtA7yb7Cr2Z+fsuhw4JbV3YY9z3iXp\ngZTf/ZLGpvh0SUskLZX0hKQrUvxyoDa1dVOKbc1p71/T+3w4HbtH61kM+4pb8XoWw77iVpzFq/N/\njy8Ut8FlsBTEu4DRkn4t6duS/heApOHAjcBZEdFIdoj3Hws1EhEbgO8CV0XEuIhYmXYdTLbYnUG2\nwOXzEWBpRPwa+KOk41J8NrAytXdVj3MeB06JiPHAJcBXcvaNA84CGoGzJI2OiNlAV2rrnNyGJH0g\n5XBCRBwDXJEvSUkzJLVKat2yZUuhW2FmFdDdMyw2boPLoCiIEbEVOA6YAWwB/k3SdGAs8LtUpAB+\nAJzaj0u0RMRfIuIx4KACx0wFbkmvb2HnYdNC9gdulfQIcBVwZM6+X0TEnyPiZeAx4G19tHU68P2I\neAkgIvJ+VY+IhRHRFBFNI0eOLCJFM9tdaqSS4ja4DIqCCBARmYhYERFfAD4NfKyPU17ltfyH93Hs\ntpzXu3wyJb0ZeA/wPUkbgFnA/5b6/BR/GVgeEUcBH+qRR+41MwyiCUzVYr831ZQUt+KdfMibS4pb\ncaaeMLqkuA0ug6IgShor6bCc0DjgKWA90CDp0BT/W+A/0+sNZHuVsHPxfAHYt8QUzgR+FBFvi4iG\niBgN/A44pY/29gc60uvpRV5ru6RheeJ3A38vaW/YUaT3aOsuff8uxc+zTMvjpnNP3KX4eZbp6zev\nuZFpE8bs6BHWSEybMMazTMug0GzScs4yHSy9ln2Ab0qqI9vz+w0wIyJelvT3ZIcl9wLWkH1GCHAp\ncL2kLwMrctr6GfATSR8Bzi/y+lOBr/aI3ZbiM4GMpIfJPs9syznmCuAHkuYCxU51Wgisk/Rg7nPE\niFgqaRzQKukV4Of0mJG6J3LxGzgufgNjXnOjC+AAKWfxy0fhh71Vq6mpKVpbWyudhplZVZG0NiKa\nesYHxZCpmZlZpbkgmpmZ4YJoZmYGuCCamZkBLohmZmaAC6KZmRnggmhmZga4IJqZmQEuiGZmZoAL\nopmZGeCCaGZmBrggmpmZAS6IZmZmQBHLP0m6CngqIr6RtpcBmyLiH9L214GOiLiylAtLugBY2L1C\nfI99K4B9uv81cklNwNciYmIJ7TcAJ0XEzWl7OtAUEZ/u47xhZBf+/RjZtRC3AV+KiP8o9to5bU0E\nXomI+/tx3sURcUap1+xLS1sHC5atZ3NnF6Pqapk1eSzN4+vLfZmymXTlCp545sUd24cdOIK7L5pY\nuYSGkLkt7SxevYlMBDUSU08Y7WWLysD3deA0zN51lb1yLglVTA/xPuAkAElvAA4AjszZfxJQ0i/8\n5AJg7172HyjpA/1ol7R2YgPw8X6c/mXgYOCoiDgWaKb0BYe7TSTdu55SjrtVS1sHc5a009HZRQAd\nnV3MWdJOS1tHn+dWQs9iCPDEMy8y6coVlUloCJnb0s6iVRvJpOXfMhEsWrWRuS3tFc6suvm+Dpx8\nxbC3eH8UUxDvB7pXEj0SeAR4QdL/kPQm4J3AgwCSZklaI2mdpEtTbISkOyU9LOkRSWdJmgmMApZL\nWl7guguAz/cMShou6fuS2iW1STotxadLul3SPcAvgMuBUyQ9JOnCdPooSUslPSHpijxt7w2cC5wf\nEdsAIuIPEfHjtP99kh6Q9KCkWyXtk+IbJF2a4u2SDk891POAC1MOp0i6UdJ3Ja0GrpD0rtRem6T7\nJY0t4v9Hvy1Ytp6u7ZmdYl3bMyxYtn4gL9tvPYthX3Er3uLVm0qKW3F8X6tbn72UiNgs6VVJY8j2\ndh4A6skWyT8D7RHxiqT3AYcB7wIE3C7pVGAksDkiPgggaf+I+LOki4DTIuLZApd+APhoKngv5MT/\nOZtWNEo6HLhL0jvSvmOBoyPiuZ7DjmnIdBwwnuww6HpJ34yI3E/qocDGiHi+ZzKSDgDmAqdHxIuS\n/hW4CPhSOuTZiDhW0j+l6/6DpO8CWyPia6mNTwFvJTuUm5G0H3BKRLwq6XTgK2SHaguSNAOYATBm\nzJjeDt3F5s6ukuI2dGUKLAxeKG7F8X2tbsVOqrmfbDHsLogP5Gzfl455X/ppI9tjPJxsgWwHJkn6\nqqRTIuLPJeQ3j2wRyvVuYBFARDwOPAV0F8S7I+K5Xtr7RUT8OSJeBh4D3lZCLhOAI4D7JD0EfKLH\n+UvSf9eSHa4t5NaI6O6m7Q/cKukR4Cp2HorOKyIWRkRTRDSNHDmyhPRhVF1tSXEbumqkkuJWHN/X\n6lZsQex+jthIdsh0FdkeYu7zQwHzI2Jc+jk0Iq6PiF+T7bm1A/MkXVJschFxD1BLthgVo6+xtG05\nrzPs2kP+DTAm9dx6EtmC2/3+joiIT+VpO1+7hXL8MrA8Io4CPgQM7yP/12XW5LHUDqvZKVY7rIZZ\nkwd0pLbfDjtwRElxK97UE0aXFLfi+L5Wt1J6iGcAz0VEJvXC6sgWxe6CuAz4ZM5ztXpJB0oaBbwU\nEYvIPhc8Nh3/AsVNVpkHfDZneyVwTrrGO4AxQL6HYMW2v0Oa8Xo9cLWkN6ZrjJT0N2S/BJws6dAU\nH5EzVFtIXznsD3TPaJleSq790Ty+nvlTGqmvq0VAfV0t86c0DtpZpndfNHGX4udZpuUxr7mRaRPG\n7Oi51EhMmzDGsyFfJ9/XgVNoNmk5Z5kWO9Oxnezs0pt7xPbpfgYYEXdJeifwgLIfhq3ANLLP5RZI\n+guwHfjHdP5CYKmkzRFxWqELR8TPJW3JCX0b+I6kduBVYHpEbNOuQxLrgIykh4EbgT8V+V7nki3C\nj0l6mWyP7pKI2JKeQy5Ok4m6j/11L239DPiJpI8A5+fZfwXwA0lzgfJNlepF8/j6QVsA83HxGzjz\nmhv9i3oA+L4OnHIWv3wUfthbtZqamqK1tbXSaZiZVRVJa7v/zj2X/6UaMzMzXBDNzMwAF0QzMzPA\nBdHMzAxwQTQzMwNcEM3MzAAXRDMzM8AF0czMDHBBNDMzA1wQzczMABdEMzMzwAXRzMwMcEE0MzMD\nil/+qSIkBXBlRPxL2r6Y7PIEbyIAAA/WSURBVJJTXyxT+w3Ar9h5PcUrI+KH/WzrjrTY7+vNayJw\ncUSc8XrbMrPdq6WtgwXL1rO5s4tRdbXMmjy2qpZcG8waZu+6Sl45l4Qa7D3EbcAUSQcM4DWejIhx\nOT8lF0MzM8gWwzlL2uno7CKAjs4u5ixpp6Wto89zrXf5imFv8f4Y7AXxVbILCV/Yc0dayf42SWvS\nz8kp3i6pTll/lPR3Kf5DSZOKvbCkrZIuk/SwpFWSDkrxQ9J2u6R5krbmObdB0kpJD6afk1J8oqQV\nkn4i6XFJNymtbCzp/Sn2IDClH/fKzCpswbL1dG3P7BTr2p5hwbL1Bc6wwWSwF0SAbwHnSNq/R/xq\n4KqIOB74GPC9FL8POBk4EvgtcEqKnwjcn6f9QyQ9lPPTffwIYFVEHAPcC5ybc92rI6IReLpAzs8A\nkyLiWOAs4JqcfeOBC4AjgL8CTpY0HLgO+BBwHPA/C90MSTMktUpq3bJlS6HDzKwCNnd2lRS3wWVQ\nP0MEiIjnJf0QmAnkfqpOB45IHSyA/STtA6wETgWeAr4DzJBUD/wpIl7Mc4knI2JcnvgrwB3p9Vqg\nu3d5ItCcXt8MfC3PucOAayWNAzLAO3L2/TIingaQ9BDQAGwFfhcRT6T4ImBGnnaJiIVke800NTVF\nvmPMrDJG1dXSkaf4jaqrrUA2Vqpq6CECfAP4FNleW7c3ABNynv3VR8RWsr25U9LPCmALcCbZQlmK\n7RHRXXAylPbl4ULgD8AxQBPwxpx923Jel9qumQ1isyaPpXZYzU6x2mE1zJo8tkIZWSmqoiBGxHPA\nj8kWxW53Aed3b6TeGBGxCTgAOCwifgv8F3Ax2UJZDqvIDtECnF3gmP2B30fEX4C/BWoKHNftcaBB\n0iFpe+rrztLMdrvm8fXMn9JIfV0tAurrapk/pdGzTMug0GzScs4yrabeydeBT+dszwS+JWkd2fdx\nL3Be2rea14rQSmA+2cKYzyFp6LLbDRFxTYFjIfv8b5GkzwNLgT/nOebbwG1pQs9SIN9Q7Q4R8bKk\nGcCdkl5KOe/b2zlmNjg1j693ARwg5Sx++ei1UUErhqS9ga6ICElnA1Mj4iOVyKWpqSlaW1srcWkz\ns6olaW1ENPWMV1MPcbA4juyEGQGdwCcrnI+ZmZWBC2KJImIl2ckyZmY2hFTFpBozM7OB5oJoZmaG\nC6KZmRnggmhmZga4IJqZmQEuiGZmZoALopmZGeCCaGZmBrggmpmZAS6IZmZmgAuimZkZ4H/LtE+S\nMkA72Xv1K+ATEfFSiW18GDgiIi6XNBK4g+yiwTOBOcDHI6KzvJkPDSdcdjd/eOGVHdsH7ftGVn9+\nUgUzKqylrYMFy9azubOLUXW1zJo8dlAvA3TonDt5NWexm70Ev5k/sMvr9Fc13dtzrnuA+558bsf2\nyYe8mZvOPbGCGfVubks7i1dvIhNBjcTUE0Yzr7mx0mnl1TD7zl1i5VwSyj3EvnVFxLiIOAp4hdfW\nXCxaRNweEZenzfcC7RExPiJWRsRfuxjm17MYAvzhhVc44bK7K5RRYS1tHcxZ0k5HZxcBdHR2MWdJ\nOy1tHZVOLa+exRDg1cjGB5tqurc9iyHAfU8+xznXPVChjHo3t6WdRas2kknLAGYiWLRqI3Nb2iuc\n2a7yFcPe4v3hglialcChAJJaJK2V9Gha3JcUf7+kByU9LOkXKTZd0rWSxgFXAB+R9JCkWkkbJB2Q\njvs7SevSuT+qwPsbVHoWw77ilbRg2Xq6tmd2inVtz7Bg2foKZdS7nsWwr3glVdO97VkM+4pX2uLV\nm0qKD3UeMi2SpL2ADwBLU+iTEfGcpFpgjaTbyH7BuA44NSJ+J+nNuW1ExEOSLgGaIuLTqd3u9o8E\n5gInRcSzPc/NyWMGMANgzJgx5X6b1k+bO7tKilvxfG8HTqbAAvGF4kOde4h9q5X0ENAKbASuT/GZ\nkh4GVgGjgcOACcC9EfE7gIgo5Wvhe4BbI+LZ3s6NiIUR0RQRTSNHjuzXG7LyG1VXW1Lciud7O3Bq\n0hfyYuNDnQti37qfIY6LiPMj4hVJE4HTgRMj4higDRhe0SyHoIP2fWNJ8UqaNXkstcNqdorVDqth\n1uSxFcqod3sV+H1XKF5J1XRvTz4k78BOwXilTT1hdEnxoc4FsX/2B/4UES9JOpxszxCyvcVTJb0d\noNCwZwH3AH8j6S39OHdIWv35SbsUv8E6y7R5fD3zpzRSX1eLgPq6WuZPaRy0MyF/M/+DuxS/wTrL\ntJru7U3nnrhL8RvMs0znNTcybcKYHT3CGolpE8YMylmmhWaTlnOWqWIPHSsulqStEbFPj9ibgBag\nAVgP1AFfjIgVkj4AfIXsl41nImKSpOmk54a5r1NbG9L2s5I+AcwCMkBbREzvLbempqZobW0t23s1\nM9sTSFobEU27xF0Qq5cLoplZ6QoVRA+ZmpmZ4YJoZmYGuCCamZkBLohmZmaAC6KZmRnggmhmZga4\nIJqZmQEuiGZmZoALopmZGeCCaGZmBrggmpmZAS6IZmZmQAULoqSQtChney9JWyTdkbY/LGl2gXO3\nFojfKOnM9HqFpF3+8dY+curO4fIe8Qsk7Z2zvUHSAaW0nXNus6Qjcra/JOn0/rRlZmbls1cFr/0i\ncJSk2ojoAiYBHd07I+J24PbdnNMk4Ndk1yWcE68tBXIBsAh4qQzXaAbuAB4DiIhLytBm0VraOliw\nbD2bO7sYVVfLrMljB+W6ct3mtrSzePUmMhHUSEw9YfSgXKutGp1z3QPc9+RzO7YH87p91fS5nXTl\nCp545sUd24cdOIK7L5pYuYSGkIbZd+4SK+d6iJUeMv050P1upgKLu3dImi7p2vT67ZIekNQuaV7O\nMZJ0raT1kv4fcGC+i0h6Xzr/QUm3Ston33Eph6uBjcCJ6dyZwChguaTledpukbRW0qOSZuTEt0q6\nTNLDklZJOkjSScCHgQWSHpJ0SI9e7fGS7k/n/FLSvsXdxuK0tHUwZ0k7HZ1dBNDR2cWcJe20tHX0\neW4lzG1pZ9GqjWTS95JMBItWbWRuS3uFM6t+PYshwH1PPsc51z1QoYwKq6bPbc9iCPDEMy8y6coV\nlUloCMlXDHuL90elC+ItwNmShgNHA6sLHHc18J2IaAR+nxP/KDAWOAL4O+Ckniemoc25wOkRcSzQ\nClyU57jhwOnAz8gW5qkAEXENsBk4LSJOy5PbJyPiOKAJmNm94j0wAlgVEccA9wLnRsT9ZHu9syJi\nXEQ8mXP9NwL/BnwmnXM60FXgfvTLgmXr6dqe2SnWtT3DgmXry3mZslm8elNJcStez2LYV7ySqulz\n27MY9hW3waWiBTEi1pFddX4q2d5iISfzWu/xRznxU4HFEZGJiM3APXnOnUC2YN4n6SHgE8Db8hx3\nBrA8Dd/eBjRLqinibcyU9DCwChgNHJbir5AdGgVYS/Z99mYs8PuIWAMQEc9HxKs9D5I0Q1KrpNYt\nW7YUkd5rNnfmr6+F4pWWKbB4daG4DU3V9rm16lXpHiJke0xfI2e4tID+/hYUcHfqkY2LiCMi4lN5\njpsKnC5pA9kC9hbgPb02LE0k25M7MfXq2oDhaff2nGeQGcr0vDYiFkZEU0Q0jRw5sqRzR9XVlhSv\ntBqppLgNTdX2ubXqNRgK4g3ApRHR24Oh+4Cz0+tzcuL3AmdJqpF0MJBvSHMVcLKkQwEkjZD0jtwD\nJO0HnAKMiYiGiGgA/pk0bAq8AOR7nrc/8KeIeEnS4WR7o30p1NZ64GBJx6ec9pVU1klPsyaPpXbY\nzp3e2mE1zJo8tpyXKZupJ4wuKW7FO/mQN5cUr6Rq+tweduCIkuI2uFS8IEbE0+k5XW8+A/yzpHYg\nd2rZT4EnyM7Y/CGwy4yAiNgCTAcWS1qXjjm8x2EfBe6JiG05sX8HPiTpTcBCYGmeSTVLgb0k/Qq4\nnGzx7cstwCxJbZIOycnzFeAs4JtpCPZuXuttlkXz+HrmT2mkvq4WAfV1tcyf0jhoZ+vNa25k2oQx\nO3qENRLTJozxLNMyuOncE3cpfoN1lmk1fW7vvmjiLsXPs0zLo9Bs0nLOMlX4eUzVampqitbW1kqn\nYWZWVSStjYhd/k694j1EMzOzwcAF0czMDBdEMzMzwAXRzMwMcEE0MzMDPMu0qknaAjzVz9MPAJ4t\nYzoDrZryraZcobryda4Dp5ryfb25vi0idvmXTVwQ91CSWvNNOx6sqinfasoVqitf5zpwqinfgcrV\nQ6ZmZma4IJqZmQEuiHuyhZVOoETVlG815QrVla9zHTjVlO+A5OpniGZmZriHaGZmBrggmpmZAS6I\neyRJ75e0XtJvJM2udD69kXSDpGckPVLpXPoiabSk5ZIek/SopM9UOqdCJA2X9EtJD6dcL610Tn1J\n6562Sbqj0rn0RdIGSe2SHpI0qJekkVQn6SeSHpf0K0mDbw2wRNLYdE+7f56XdEHZ2vczxD2LpBrg\n18Ak4GlgDTA1Ih6raGIFSDoV2Ar8MCKOqnQ+vUmLVB8cEQ9K2hdYCzQPxnsrScCIiNgqaRjwX8Bn\nIqKYNT0rQtJFQBOwX0ScUel8eiNpA9AUEYP+D90l/QBYGRHfk/RGYO+I6Kx0Xn1Jv8s6gBMior//\nQMlO3EPc87wL+E1E/DYtSnwL8JEK51RQRNwLPFfpPIoREb+PiAfT6xeAX7HzgtaDRmRtTZvD0s+g\n/XYs6a3AB4HvVTqXoUTS/sCpwPWQXai8Goph8l7gyXIVQ3BB3BPVA5tytp9mkP7SrmaSGoDxwOrK\nZlJYGoJ8CHgGuDsiBm2uwDeAzwJ/qXQiRQrgLklrJc2odDK9eDuwBfh+Go7+nqQRlU6qSGcDi8vZ\noAuiWZlJ2ge4DbggIp6vdD6FREQmIsYBbwXeJWlQDklLOgN4JiLWVjqXErw7Io4FPgD8cxr6H4z2\nAo4FvhMR44EXgUE9rwAgDe1+GLi1nO26IO55OoDROdtvTTErg/Q87jbgpohYUul8ipGGyJYD7690\nLgWcDHw4PZe7BXiPpEWVTal3EdGR/vsM8FOyjyoGo6eBp3NGB35CtkAOdh8AHoyIP5SzURfEPc8a\n4DBJb0/fss4Gbq9wTkNCmqhyPfCriLiy0vn0RtJISXXpdS3ZSVaPVzar/CJiTkS8NSIayH5e74mI\naRVOqyBJI9KkKtLw4/uAQTlLOiL+G9gkaWwKvRcYdJPA8phKmYdLIdtdtj1IRLwq6dPAMqAGuCEi\nHq1wWgVJWgxMBA6Q9DTwhYi4vrJZFXQy8LdAe3o2B/C5iPh5BXMq5GDgB2mm3huAH0fEoP9zhipx\nEPDT7Pcj9gJujoillU2pV+cDN6UvyL8F/r7C+fQqfcmYBPyfsrftP7swMzPzkKmZmRnggmhmZga4\nIJqZmQEuiGZmZoALopmZGeCCaGZmBrggmpmZAfD/AT9m7a0w2mjHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZo2hHq-fQ2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "85c6b835-0e27-4471-db1f-b9b0c59bfe28"
      },
      "source": [
        "plt.scatter(x=X_train_encoded['Travel_Distance'], y=y_train)\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcQAAAD4CAYAAABykJZ9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAPD/AGr9le3uPBqyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwwsPvjMfg82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "830c86bf-3dab-4f79-b849-5bf5633c78b9"
      },
      "source": [
        "plt.scatter(x=X_train_encoded['Travel_Distance'], y=X_train_encoded['Main_Dish'])\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIHEENQAkjqAGgMQR1ACQOIIaABL3/7zh0zgH3DtaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW99-lLVdzFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ed99e583-069c-4808-fe0c-e9bbdc5d3cfa"
      },
      "source": [
        "#%% Logistic Regression Model\n",
        "\n",
        "model = LogisticRegression(solver='lbfgs', max_iter = 10000, multi_class='auto')\n",
        "model.fit(X_train_encoded, y_train)\n",
        "loocv(X_train_encoded, y_train, model)\n",
        "#26.6% accurate\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2b31429f29a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloocv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#26.6% accurate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e9094bea45ce>\u001b[0m in \u001b[0;36mloocv\u001b[0;34m(X_train_encoded, y_train, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloocv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeaveOneOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloocv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.3f%% (%.3f%%)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 232\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1604\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1606\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 iprint=iprint, pgtol=tol, maxiter=max_iter)\n\u001b[0m\u001b[1;32m    945\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"warnflag\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 199\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    201\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, *args)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_multi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_multinomial_loss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss_grad\u001b[0;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    342\u001b[0m     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),\n\u001b[1;32m    343\u001b[0m                     dtype=X.dtype)\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m     \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss\u001b[0;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msquared_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cAuTjGLeDV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Random Forest Model\n",
        "\n",
        "classifier = RandomForestClassifier()\n",
        "features = select_features(X_train_encoded, y_train, 10)\n",
        "X_train_encoded = X_train_encoded[features]\n",
        "classifier.fit(X_train_encoded, y_train)\n",
        "loocv(X_train_encoded, y_train, classifier)\n",
        "#22.2% accurate\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzgt-k0SalNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% those were terrible, do better! Randomized Search for hyperparameters\n",
        "\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(results['mean_test_score'][candidate], results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "#%%\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {'max_depth': range(10, 50, 2),\n",
        "              'min_samples_leaf': [1,2,3,4,5],'min_samples_split': [2, 3, 4, 5],\n",
        "              'n_estimators': range(100, 500, 10),\n",
        "              \"criterion\": [\"gini\", \"entropy\"]}\n",
        "\n",
        "n_iter_search = 25\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, cv=5, iid=False, n_jobs=-1)\n",
        "\n",
        "random_search.fit(X_train_encoded, y_train)\n",
        "\n",
        "report(random_search.cv_results_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq2oQkU8aEsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Grid Search for hyperparameters\n",
        "\n",
        "param_grid = {\"max_depth\": range(30, 50, 5),\n",
        "              \"n_estimators\": range(100, 500, 10),\n",
        "              \"min_samples_leaf\": range(2, 5)\n",
        "              }\n",
        "\n",
        "# run grid search\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=3, iid=False, n_jobs=-1)\n",
        "grid_search.fit(X_train_encoded, y_train)\n",
        "report(grid_search.cv_results_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBZ0AyQ_aUAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% this model is awful, let's try a new target\n",
        "#predict whether someone will have Turkey, Ham, or something else based on other items?\n",
        "\n",
        "train['Main_Dish'].unique()\n",
        "train['Main_Dish'].value_counts(normalize=True)\n",
        "\n",
        "#baseline = Turkey, 86.7%\n",
        "#%%\n",
        "target = 'Main_Dish'\n",
        "X_train_encoded, X_test_encoded, y_train, y_test = X_y_dataframes(train, test, target)\n",
        "\n",
        "#%%\n",
        "def select_features(X_train_encoded, y_train, num_features):\n",
        "    classifier = RandomForestClassifier(n_estimators=100)\n",
        "    rfe_selector = RFE(estimator=classifier, n_features_to_select=num_features, step=10, verbose=5)\n",
        "    rfe_selector.fit(X_train_encoded, y_train)\n",
        "    rfe_support = rfe_selector.get_support()\n",
        "    rfe_feature = X_train_encoded.loc[:,rfe_support].columns.tolist()\n",
        "    print(str(len(rfe_feature)), 'selected features')\n",
        "    print(rfe_feature)\n",
        "    return rfe_feature\n",
        "#%%\n",
        "classifier = RandomForestClassifier(n_estimators=100)\n",
        "features = select_features(X_train_encoded, y_train, 10)\n",
        "X_train_encoded = X_train_encoded[features]\n",
        "classifier.fit(X_train_encoded, y_train)\n",
        "loocv(X_train_encoded, y_train, classifier)\n",
        "\n",
        "#%% but I'd really like this to be about the food, not region or age\n",
        "\n",
        "food = ['Stuffing', 'Cranberry_Sauce', 'Gravy', 'Brussel_Sprouts', 'Carrots', 'Cauliflower',\n",
        "       'Corn', 'Cornbread', 'Fruit_Salad', 'Green_Beans', 'Mac_and_Cheese',\n",
        "       'Mashed_Potatoes', 'Rolls_Biscuits', 'Squash', 'Salad',\n",
        "       'Sweet_Potatoes', 'Apple_Pie', 'Buttermilk_Pie', 'Cherry_Pie',\n",
        "       'Chocolate_Pie', 'Coconut_Cream_Pie', 'Key_Lime_Pie', 'Peach_Pie',\n",
        "       'Pecan_Pie', 'Pumpkin_Pie', 'Sweet_Potato_Pie', 'No_Pie',\n",
        "       'Apple_Cobbler', 'Blondies', 'Brownies', 'Carrot_Cake', 'Cheesecake',\n",
        "       'Cookies', 'Fudge', 'Ice_Cream', 'Peach_Cobbler']\n",
        "#%%\n",
        "        \n",
        "target = 'Main_Dish'\n",
        "X_train_encoded, X_test_encoded, y_train, y_test = X_y_dataframes(train, test, target)\n",
        "    \n",
        "\n",
        "#%%\n",
        "for column in X_train_encoded.columns:\n",
        "    if column not in food:\n",
        "        X_train_encoded = X_train_encoded.drop(column, axis=1)\n",
        "\n",
        "\n",
        "#%%\n",
        "model = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter = 10000)\n",
        "model.fit(X_train_encoded, y_train)\n",
        "loocv(X_train_encoded, y_train, model)\n",
        "#%%\n",
        "\n",
        "classifier = RandomForestClassifier(n_estimators=100)\n",
        "classifier.fit(X_train_encoded, y_train)\n",
        "loocv(X_train_encoded, y_train, classifier)\n",
        "\n",
        "#%%\n",
        "classifier = RandomForestClassifier(n_estimators=480, max_depth=45, min_samples_leaf=3)\n",
        "features = select_features(X_train_encoded, y_train, 10)\n",
        "X_train_encoded = X_train_encoded[features]\n",
        "classifier.fit(X_train_encoded, y_train)\n",
        "loocv(X_train_encoded, y_train, classifier)\n",
        "\n",
        "#%%\n",
        "target = 'Black_Friday_Shopper'\n",
        "train['Black_Friday_Shopper'].value_counts(normalize=True)\n",
        "#%%\n",
        "X_train_encoded, X_test_encoded, y_train, y_test = X_y_dataframes(train, test, target)\n",
        "#%%\n",
        "\n",
        "X_train_encoded = X_train_encoded[features]\n",
        "classifier.fit(X_train_encoded, y_train)\n",
        "loocv(X_train_encoded, y_train, classifier)\n",
        "#%%\n",
        "param_grid = {\"max_depth\": range(30, 50, 5),\n",
        "              \"n_estimators\": range(100, 500, 10),\n",
        "              \"min_samples_leaf\": range(2, 5)\n",
        "              }\n",
        "\n",
        "# run grid search\n",
        "grid_search = GridSearchCV(classifier, param_grid=param_grid, cv=3, iid=False, n_jobs=-1)\n",
        "grid_search.fit(X_train_encoded, y_train)\n",
        "report(grid_search.cv_results_)\n",
        "#%%\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "y_pred_train = classifier.predict(X_train_encoded)\n",
        "print(precision_recall_fscore_support(y_train, y_pred_train, average = 'weighted'))\n",
        "\n",
        "#try gradient boosted trees!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1x_OzE9ar21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Random Forest Model updated\n",
        "\n",
        "\n",
        "classifier = RandomForestClassifier(n_estimators = 380, min_samples_split = 4, \n",
        "                                    min_samples_leaf = 2, max_depth = 24, criterion = 'gini')\n",
        "classifier.fit(X_train_encoded, y_train)\n",
        "loocv(X_train_encoded, y_train, classifier)\n",
        "#28.3 accuracy\n",
        "\n",
        "\n",
        "#%%find features\n",
        "\n",
        "rfe_selector = RFE(estimator=RandomForestClassifier(n_estimators = 350, min_samples_split = 2, \n",
        "                                    min_samples_leaf = 3, max_depth = 46, criterion = 'entropy'), n_features_to_select=10, step=10, verbose=5)\n",
        "rfe_selector.fit(X_train_encoded, y_train)\n",
        "rfe_support = rfe_selector.get_support()\n",
        "rfe_feature = X_train_encoded.loc[:,rfe_support].columns.tolist()\n",
        "print(str(len(rfe_feature)), 'selected features')\n",
        "print(rfe_feature)\n",
        "#%%create model just those features\n",
        "features = rfe_feature\n",
        "\n",
        "X_train_encoded = X_train_encoded[features]\n",
        "X_test_encoded = X_test_encoded[features]\n",
        "\n",
        "classifier = RandomForestClassifier(n_estimators = 460, min_samples_split = 3, \n",
        "                                    min_samples_leaf = 5, max_depth = 40, criterion = 'entropy')\n",
        "classifier.fit(X_train_encoded, y_train)\n",
        "loocv(X_train_encoded, y_train, classifier)\n",
        "#%% other measures of \"how good\" my model is\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "y_pred_train = classifier.predict(X_train_encoded)\n",
        "print(precision_recall_fscore_support(y_train, y_pred_train, average = 'weighted'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dcIcj4MsaXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3e144de-305d-4923-d5c2-210633e39fa9"
      },
      "source": [
        "import xgboost\n",
        "model = xgboost.XGBClassifier(n_jobs=-1)\n",
        "model.fit(X_train_encoded, y_train)\n",
        "\n",
        "\n",
        "loocv(X_train_encoded, y_train, model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 29.796% (45.736%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLxRBOCWvFgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "85021f63-2602-4e5e-faa7-2a9c054ff815"
      },
      "source": [
        "!pip install eli5"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting eli5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/2f/c85c7d8f8548e460829971785347e14e45fa5c6617da374711dec8cb38cc/eli5-0.10.1-py2.py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |███                             | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 81kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 92kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from eli5) (1.3.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from eli5) (1.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from eli5) (2.10.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (19.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.21.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.8.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->eli5) (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->eli5) (0.14.0)\n",
            "Installing collected packages: eli5\n",
            "Successfully installed eli5-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2b5r5q8slNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "af30fa14-9825-48bf-c44a-5c5fcfec1a2a"
      },
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC().fit(X_train_encoded, y_train)\n",
        "perm = PermutationImportance(svc).fit(X_train_encoded, y_train)\n",
        "eli5.show_weights(perm)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0939\n",
              "                \n",
              "                    &plusmn; 0.0099\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x42\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 85.13%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0615\n",
              "                \n",
              "                    &plusmn; 0.0202\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x51\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.13%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0248\n",
              "                \n",
              "                    &plusmn; 0.0090\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x0\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.25%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0242\n",
              "                \n",
              "                    &plusmn; 0.0118\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x1\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.44%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0234\n",
              "                \n",
              "                    &plusmn; 0.0083\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x48\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 93.06%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0207\n",
              "                \n",
              "                    &plusmn; 0.0063\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x49\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 93.78%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0177\n",
              "                \n",
              "                    &plusmn; 0.0102\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x3\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 95.34%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0117\n",
              "                \n",
              "                    &plusmn; 0.0106\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x40\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 95.89%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0098\n",
              "                \n",
              "                    &plusmn; 0.0047\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x2\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 96.30%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0084\n",
              "                \n",
              "                    &plusmn; 0.0036\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x10\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 96.47%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0079\n",
              "                \n",
              "                    &plusmn; 0.0044\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x18\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 96.73%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0071\n",
              "                \n",
              "                    &plusmn; 0.0027\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x26\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 96.81%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0068\n",
              "                \n",
              "                    &plusmn; 0.0034\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x9\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0063\n",
              "                \n",
              "                    &plusmn; 0.0050\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x21\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0063\n",
              "                \n",
              "                    &plusmn; 0.0013\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x25\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.28%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0054\n",
              "                \n",
              "                    &plusmn; 0.0054\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x13\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.37%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0052\n",
              "                \n",
              "                    &plusmn; 0.0050\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x12\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.47%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0049\n",
              "                \n",
              "                    &plusmn; 0.0028\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x50\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.57%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0046\n",
              "                \n",
              "                    &plusmn; 0.0066\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x44\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.67%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0044\n",
              "                \n",
              "                    &plusmn; 0.0032\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                x41\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 97.67%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 32 more &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jAKMuO8vSbC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a1a8b334-b06b-47ae-e60f-1bfd80bfb3f4"
      },
      "source": [
        "X_train_encoded.iloc[:,42]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      11.0\n",
              "1       0.0\n",
              "2       0.0\n",
              "3       2.0\n",
              "4       0.0\n",
              "       ... \n",
              "730     4.0\n",
              "731     2.0\n",
              "732     0.0\n",
              "733     0.0\n",
              "734     0.0\n",
              "Name: Kids_Table_Age, Length: 735, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkjrktZ7vtai",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d329f728-b1c7-4484-dd8a-40c43e83783e"
      },
      "source": [
        "X_train_encoded.iloc[:,51]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      3.0\n",
              "1      8.0\n",
              "2      2.0\n",
              "3      2.0\n",
              "4      7.0\n",
              "      ... \n",
              "730    0.0\n",
              "731    8.0\n",
              "732    2.0\n",
              "733    9.0\n",
              "734    6.0\n",
              "Name: Household_Earnings, Length: 735, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}