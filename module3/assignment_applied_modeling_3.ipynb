{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCc3XZEyG3XV"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 3*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Applied Modeling, Module 3\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Complete these tasks for your project, and document your work.\n",
    "\n",
    "- [ ] Continue to iterate on your project: data cleaning, exploration, feature engineering, modeling.\n",
    "- [ ] Make at least 1 partial dependence plot to explain your model.\n",
    "- [ ] Share at least 1 visualization on Slack.\n",
    "\n",
    "(If you have not yet completed an initial model yet for your portfolio project, then do today's assignment using your Tanzania Waterpumps model.)\n",
    "\n",
    "## Stretch Goals\n",
    "- [ ] Make multiple PDPs with 1 feature in isolation.\n",
    "- [ ] Make multiple PDPs with 2 features in interaction. \n",
    "- [ ] Use Plotly to make a 3D PDP.\n",
    "- [ ] Make PDPs with categorical feature(s). Use Ordinal Encoder, outside of a pipeline, to encode your data first. If there is a natural ordering, then take the time to encode it that way, instead of random integers. Then use the encoded data with pdpbox. Get readable category names on your plot, instead of integer category codes.\n",
    "\n",
    "## Links\n",
    "- [Christoph Molnar: Interpretable Machine Learning — Partial Dependence Plots](https://christophm.github.io/interpretable-ml-book/pdp.html) + [animated explanation](https://twitter.com/ChristophMolnar/status/1066398522608635904)\n",
    "- [Kaggle / Dan Becker: Machine Learning Explainability — Partial Dependence Plots](https://www.kaggle.com/dansbecker/partial-plots)\n",
    "- [Plotly: 3D PDP example](https://plot.ly/scikit-learn/plot-partial-dependence/#partial-dependence-of-house-value-on-median-age-and-average-occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "    !pip install eli5\n",
    "    !pip install pdpbox\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 122)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/Users/maxefremov/Workshop/data/home-credit-default-risk/'\n",
    "application_train = 'application_train.csv'\n",
    "\n",
    "df = pd.read_csv(path + application_train)\n",
    "print(df.shape)\n",
    "\n",
    "# Split train into train & val\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(df, train_size=0.80, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "A majority class baseline model would have us predict that every loan applicant will pay back their loan with no difficulty, assigning 0s for every observation in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.919271\n",
       "1    0.080729\n",
       "Name: TARGET, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TARGET'].value_counts(normalize=True)\n",
    "# Seems like the data is unbalanced, in that only 8% of observations are of target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An XGBClassifier model with minimal-to-no feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'TARGET'\n",
    "\n",
    "# Get a dataframe with all train columns except the target\n",
    "train_features = df.drop(columns=[target])\n",
    "\n",
    "# Get a list of the numeric features\n",
    "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Get a series with the cardinality of the nonnumeric features\n",
    "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
    "\n",
    "# Get a list of all categorical features with cardinality <= 50\n",
    "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
    "\n",
    "# Combine the lists \n",
    "features = numeric_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange data into X features matrix and y target vector \n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_val = val[features]\n",
    "y_val = val[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached https://files.pythonhosted.org/packages/96/84/4e2cae6247f397f83d8adc5c2a2a0c5d7d790a14a4c7400ff6574586f589/xgboost-0.90.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "encoder = ce.OrdinalEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_val_encoded = encoder.fit_transform(X_val[features])\n",
    "\n",
    "X_train_encoded.shape, X_val_encoded.shape\n",
    "\n",
    "eval_set = [(X_train_encoded, y_train), (X_val_encoded,y_val)]\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=7,\n",
    "    learning_rate=.1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_encoded,y_train, eval_set=eval_set,\n",
    "         eval_metric='auc',early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ROC AUC score for the class with index -1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_proba = model.predict_proba(X_val_encoded)[:, -1]\n",
    "print('Validation ROC AUC', roc_auc_score(y_val, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = model.evals_result()\n",
    "train_error = results['validation_0']['auc']\n",
    "val_error = results['validation_1']['auc']\n",
    "epoch = range(1, len(train_error)+1)\n",
    "plt.plot(epoch, train_error, label='Train')\n",
    "plt.plot(epoch, val_error, label='Validation')\n",
    "plt.ylabel('Classification Error')\n",
    "plt.xlabel('Model Complexity (n_estimators)')\n",
    "plt.ylim((.5,1)) # Zoom in\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Dependence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, when you save matplotlib images to include in blog posts or web apps,\n",
    "# increase the dots per inch (double it), so the text isn't so fuzzy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = pd.DataFrame(data=df.isnull().sum())\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls.iloc[0:125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdpbox.pdp import pdp_isolate, pdp_plot\n",
    "\n",
    "feature = 'AMT_CREDIT'\n",
    "\n",
    "isolated = pdp_isolate(\n",
    "    model=model, \n",
    "    dataset=X_val, \n",
    "    model_features=X_val[numeric_features], \n",
    "    feature=feature\n",
    ")\n",
    "\n",
    "pdp_plot(isolated, feature_name=feature);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_applied_modeling_3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
