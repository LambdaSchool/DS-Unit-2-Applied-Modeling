{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"#### Set up the dataset, features, and targets ####"},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"data":{"text/plain":"NGXBTU    2382\ndtype: int64"},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":"import pandas\nimport numpy\n\nrecs = pandas.read_csv('./Project-2/data/recs2015_public_v4.csv')\nisna = recs.isna().sum()\nisna[isna>0]"},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":"import pandas\nimport numpy\n\nrecs = pandas.read_csv('./Project-2/data/recs2015_public_v4.csv')\n\n# Strictly speaking, neither of these are necesarry\n# but they might come in useful for future data or models\nrecs['NGXBTU'] = recs['NGXBTU'].fillna(0)\nrecs['CLIMATE_REGION_PUB'] = recs['CLIMATE_REGION_PUB'].fillna('Unknown')\n\n# These are the initial targets we'll be predicting\npotential_targets = ['TOTALBTU',\n\t\t\t\t\t'TOTALDOL',\n\t\t\t\t\t'KWH',\n\t\t\t\t\t'BTUEL',\n\t\t\t\t\t'DOLLAREL',\n\t\t\t\t\t'CUFEETNG',\n\t\t\t\t\t'BTUNG',\n\t\t\t\t\t'DOLLARNG',\n\t\t\t\t\t'GALLONLP',\n\t\t\t\t\t'BTULP',\n\t\t\t\t\t'DOLLARLP',\n\t\t\t\t\t'GALLONFO',\n\t\t\t\t\t'BTUFO',\n\t\t\t\t\t'DOLLARFO',\n\t\t\t\t\t'WOODAMT',\n\t\t\t\t\t'WOODBTU']\n\n# These are the features in the data set with leakage\nnot_features = [\"KWH\",\t# Numeric\t8\tTotal site electricity usage, in kilowatthours, 2015\n\t\t\t\t\"KWHSPH\",\t# Numeric\t8\tElectricity usage for space heating, main and secondary, in kilowatthours, 2015\n\t\t\t\t\"KWHCOL\",\t# Numeric\t8\tElectricity usage for air conditioning (central systems and individual units), in kilowatthours, 2015\n\t\t\t\t\"KWHWTH\",\t# Numeric\t8\tElectricity usage for water heating, main and secondary, in kilowatthours, 2015\n\t\t\t\t\"KWHRFG\",\t# Numeric\t8\tElectricity usage for all refrigerators, in kilowatthours, 2015\n\t\t\t\t\"KWHRFG1\",\t# Numeric\t8\tElectricity usage for first refrigerators, in kilowatthours, 2015\n\t\t\t\t\"KWHRFG2\",\t# Numeric\t8\tElectricity usage for second refrigerators, in kilowatthours, 2015\n\t\t\t\t\"KWHFRZ\",\t# Numeric\t8\tElectricity usage for freezers, in kilowatthours, 2015\n\t\t\t\t\"KWHCOK\",\t# Numeric\t8\tElectricity usage for cooking (stoves, cooktops, and ovens), in kilowatthours, 2015\n\t\t\t\t\"KWHMICRO\",\t# Numeric\t8\tElectricity usage for microwaves, in kilowatthours, 2015\n\t\t\t\t\"KWHCW\",\t# Numeric\t8\tElectricity usage for clothes washers, in kilowatthours, 2015\n\t\t\t\t\"KWHCDR\",\t# Numeric\t8\tElectricity usage for clothes dryers, in kilowatthours, 2015\n\t\t\t\t\"KWHDWH\",\t# Numeric\t8\tElectricity usage for dishwashers, in kilowatthours, 2015\n\t\t\t\t\"KWHLGT\",\t# Numeric\t8\tElectricity usage for indoor and outdoor lighting, in kilowatthours, 2015\n\t\t\t\t\"KWHTVREL\",\t# Numeric\t8\tElectricity usage for all televisions and related peripherals, in kilowatthours, 2015\n\t\t\t\t\"KWHTV1\",\t# Numeric\t8\tElectricity usage for first televisions, in kilowatthours, 2015\n\t\t\t\t\"KWHTV2\",\t# Numeric\t8\tElectricity usage for second televisions, in kilowatthours, 2015\n\t\t\t\t\"KWHAHUHEAT\",\t# Numeric\t8\tElectricity usage for air handlers and boiler pumps used for heating, in kilowatthours, 2015\n\t\t\t\t\"KWHAHUCOL\",\t# Numeric\t8\tElectricity usage for air handlers used for cooling, in kilowatthours, 2015\n\t\t\t\t\"KWHEVAPCOL\",\t# Numeric\t8\tElectricity usage for evaporative coolers, in kilowatthours, 2015\n\t\t\t\t\"KWHCFAN\",\t# Numeric\t8\tElectricity usage for ceiling fans, in kilowatthours, 2015\n\t\t\t\t\"KWHDHUM\",\t# Numeric\t8\tElectricity usage for dehumidifiers, in kilowatthours, 2015\n\t\t\t\t\"KWHHUM\",\t# Numeric\t8\tElectricity usage for humidifiers, in kilowatthours, 2015\n\t\t\t\t\"KWHPLPMP\",\t# Numeric\t8\tElectricity usage for swimming pool pumps, in kilowatthours, 2015\n\t\t\t\t\"KWHHTBPMP\",\t# Numeric\t8\tElectricity usage for hot tub pumps, in kilowatthours, 2015\n\t\t\t\t\"KWHHTBHEAT\",\t# Numeric\t8\tElectricity usage for hot tub heaters, in kilowatthours, 2015\n\t\t\t\t\"KWHNEC\",\t# Numeric\t8\tElectricity usage for other devices and purposes not elsewhere classified, in kilowatthours, 2015\n\t\t\t\t\"BTUEL\",\t# Numeric\t8\tTotal site electricity usage, in thousand Btu, 2015\n\t\t\t\t\"BTUELSPH\",\t# Numeric\t8\tElectricity usage for space heating, main and secondary, in thousand Btu, 2015\n\t\t\t\t\"BTUELCOL\",\t# Numeric\t8\tElectricity usage for air conditioning (central systems and individual units), in thousand Btu, 2015\n\t\t\t\t\"BTUELWTH\",\t# Numeric\t8\tElectricity usage for water heating, main and secondary, in thousand Btu, 2015\n\t\t\t\t\"BTUELRFG\",\t# Numeric\t8\tElectricity usage for all refrigerators, in thousand Btu, 2015\n\t\t\t\t\"BTUELRFG1\",\t# Numeric\t8\tElectricity usage for first refrigerators, in thousand Btu, 2015\n\t\t\t\t\"BTUELRFG2\",\t# Numeric\t8\tElectricity usage for second refrigerators, in thousand Btu, 2015\n\t\t\t\t\"BTUELFRZ\",\t# Numeric\t8\tElectricity usage for freezers, in thousand Btu, 2015\n\t\t\t\t\"BTUELCOK\",\t# Numeric\t8\tElectricity usage for cooking (stoves, cooktops, and ovens), in thousand Btu, 2015\n\t\t\t\t\"BTUELMICRO\",\t# Numeric\t8\tElectricity usage for microwaves, in thousand Btu, 2015\n\t\t\t\t\"BTUELCW\",\t# Numeric\t8\tElectricity usage for clothes washers, in thousand Btu, 2015\n\t\t\t\t\"BTUELCDR\",\t# Numeric\t8\tElectricity usage for clothes dryers, in thousand Btu, 2015\n\t\t\t\t\"BTUELDWH\",\t# Numeric\t8\tElectricity usage for dishwashers, in thousand Btu, 2015\n\t\t\t\t\"BTUELLGT\",\t# Numeric\t8\tElectricity usage for indoor and outdoor lighting, in thousand Btu, 2015\n\t\t\t\t\"BTUELTVREL\",\t# Numeric\t8\tElectricity usage for all televisions and related peripherals, in thousand Btu, 2015\n\t\t\t\t\"BTUELTV1\",\t# Numeric\t8\tElectricity usage for first televisions, in thousand Btu, 2015\n\t\t\t\t\"BTUELTV2\",\t# Numeric\t8\tElectricity usage for second televisions, in thousand Btu, 2015\n\t\t\t\t\"BTUELAHUHEAT\",\t# Numeric\t8\tElectricity usage for air handlers and boiler pumps used for heating, in thousand Btu, 2015\n\t\t\t\t\"BTUELAHUCOL\",\t# Numeric\t8\tElectricity usage for air handlers used for cooling, in thousand Btu, 2015\n\t\t\t\t\"BTUELEVAPCOL\",\t# Numeric\t8\tElectricity usage for evaporative coolers, in thousand Btu, 2015\n\t\t\t\t\"BTUELCFAN\",\t# Numeric\t8\tElectricity usage for ceiling fans, in thousand Btu, 2015\n\t\t\t\t\"BTUELDHUM\",\t# Numeric\t8\tElectricity usage for dehumidifiers, in thousand Btu, 2015\n\t\t\t\t\"BTUELHUM\",\t# Numeric\t8\tElectricity usage for humidifiers, in thousand Btu, 2015\n\t\t\t\t\"BTUELPLPMP\",\t# Numeric\t8\tElectricity usage for swimming pool pumps, in thousand Btu, 2015\n\t\t\t\t\"BTUELHTBPMP\",\t# Numeric\t8\tElectricity usage for hot tub pumps, in thousand Btu, 2015\n\t\t\t\t\"BTUELHTBHEAT\",\t# Numeric\t8\tElectricity usage for hot tub heaters, in thousand Btu, 2015\n\t\t\t\t\"BTUELNEC\",\t# Numeric\t8\tElectricity usage for other devices and purposes not elsewhere classified, in thousand Btu, 2015\n\t\t\t\t\"DOLLAREL\",\t# Numeric\t8\tTotal electricity cost, in dollars, 2015\n\t\t\t\t\"DOLELSPH\",\t# Numeric\t8\tElectricity cost for space heating, main and secondary, in dollars, 2015\n\t\t\t\t\"DOLELCOL\",\t# Numeric\t8\tElectricity cost for air conditioning (central systems and individual units), in dollars, 2015\n\t\t\t\t\"DOLELWTH\",\t# Numeric\t8\tElectricity cost for water heating, main and secondary, in dollars, 2015\n\t\t\t\t\"DOLELRFG\",\t# Numeric\t8\tElectricity cost for all refrigerators, in dollars, 2015\n\t\t\t\t\"DOLELRFG1\",\t# Numeric\t8\tElectricity cost for first refrigerators, in dollars, 2015\n\t\t\t\t\"DOLELRFG2\",\t# Numeric\t8\tElectricity cost for second refrigerators, in dollars, 2015\n\t\t\t\t\"DOLELFRZ\",\t# Numeric\t8\tElectricity cost for freezers, in dollars, 2015\n\t\t\t\t\"DOLELCOK\",\t# Numeric\t8\tElectricity cost for cooking (stoves, cooktops, and ovens), in dollars, 2015\n\t\t\t\t\"DOLELMICRO\",\t# Numeric\t8\tElectricity cost for microwaves, in dollars, 2015\n\t\t\t\t\"DOLELCW\",\t# Numeric\t8\tElectricity cost for clothes washers, in dollars, 2015\n\t\t\t\t\"DOLELCDR\",\t# Numeric\t8\tElectricity cost for clothes dryers, in dollars, 2015\n\t\t\t\t\"DOLELDWH\",\t# Numeric\t8\tElectricity cost for dishwashers, in dollars, 2015\n\t\t\t\t\"DOLELLGT\",\t# Numeric\t8\tElectricity cost for indoor and outdoor lighting, in dollars, 2015\n\t\t\t\t\"DOLELTVREL\",\t# Numeric\t8\tElectricity cost for all televisions and related peripherals, in dollars, 2015\n\t\t\t\t\"DOLELTV1\",\t# Numeric\t8\tElectricity cost for first televisions, in dollars, 2015\n\t\t\t\t\"DOLELTV2\",\t# Numeric\t8\tElectricity cost for second televisions, in dollars, 2015\n\t\t\t\t\"DOLELAHUHEAT\",\t# Numeric\t8\tElectricity cost for air handlers and boiler pumps used for heating, in dollars, 2015\n\t\t\t\t\"DOLELAHUCOL\",\t# Numeric\t8\tElectricity cost for air handlers used for cooling, in dollars, 2015\n\t\t\t\t\"DOLELEVAPCOL\",\t# Numeric\t8\tElectricity cost for evaporative coolers, in dollars, 2015\n\t\t\t\t\"DOLELCFAN\",\t# Numeric\t8\tElectricity cost for ceiling fans, in dollars, 2015\n\t\t\t\t\"DOLELDHUM\",\t# Numeric\t8\tElectricity cost for dehumidifiers, in dollars, 2015\n\t\t\t\t\"DOLELHUM\",\t# Numeric\t8\tElectricity cost for humidifiers, in dollars, 2015\n\t\t\t\t\"DOLELPLPMP\",\t# Numeric\t8\tElectricity cost for swimming pool pumps, in dollars, 2015\n\t\t\t\t\"DOLELHTBPMP\",\t# Numeric\t8\tElectricity cost for hot tub pumps, in dollars, 2015\n\t\t\t\t\"DOLELHTBHEAT\",\t# Numeric\t8\tElectricity cost for hot tub heaters, in dollars, 2015\n\t\t\t\t\"DOLELNEC\",\t# Numeric\t8\tElectricity cost for other devices and purposes not elsewhere classified, in dollars, 2015\n\t\t\t\t\"CUFEETNG\",\t# Numeric\t8\tTotal natural gas usage, in hundred cubic feet, 2015\n\t\t\t\t\"CUFEETNGSPH\",\t# Numeric\t8\tNatural gas usage for space heating, main and secondary, in hundred cubic feet, 2015\n\t\t\t\t\"CUFEETNGWTH\",\t# Numeric\t8\tNatural gas usage for water heating , main and secondary, in hundred cubic feet, 2015\n\t\t\t\t\"CUFEETNGCOK\",\t# Numeric\t8\tNatural gas usage for cooking (stoves, cooktops, and ovens), in hundred cubic feet, 2015\n\t\t\t\t\"CUFEETNGCDR\",\t# Numeric\t8\tNatural gas usage for clothes dryers, in hundred cubic feet, 2015\n\t\t\t\t\"CUFEETNGPLHEAT\",\t# Numeric\t8\tNatural gas usage for swimming pool heaters, in hundred cubic feet, 2015\n\t\t\t\t\"CUFEETNGHTBHEAT\",\t# Numeric\t8\tNatural gas usage for hot tub heaters, in hundred cubic feet, 2015\n\t\t\t\t\"CUFEETNGNEC\",\t# Numeric\t8\tNatural gas usage for other devices and purposes not elsewhere classified, in hundred cubic feet, 2015\n\t\t\t\t\"BTUNG\",\t# Numeric\t8\tTotal natural gas usage, in thousand Btu, 2015\n\t\t\t\t\"BTUNGSPH\",\t# Numeric\t8\tNatural gas usage for space heating, main and secondary, in thousand Btu, 2015\n\t\t\t\t\"BTUNGWTH\",\t# Numeric\t8\tNatural gas usage for water heating, main and secondary, in thousand Btu, 2015\n\t\t\t\t\"BTUNGCOK\",\t# Numeric\t8\tNatural gas usage for cooking (stoves, cooktops, and ovens), in thousand Btu, 2015\n\t\t\t\t\"BTUNGCDR\",\t# Numeric\t8\tNatural gas usage for clothes dryers, in thousand Btu, 2015\n\t\t\t\t\"BTUNGPLHEAT\",\t# Numeric\t8\tNatural gas usage for swimming pool heaters, in thousand Btu, 2015\n\t\t\t\t\"BTUNGHTBHEAT\",\t# Numeric\t8\tNatural gas usage for hot tub heaters, in thousand Btu, 2015\n\t\t\t\t\"BTUNGNEC\",\t# Numeric\t8\tNatural gas usage for other devices and purposes not elsewhere classified, in thousand Btu, 2015\n\t\t\t\t\"DOLLARNG\",\t# Numeric\t8\tTotal natural gas cost, in dollars, 2015\n\t\t\t\t\"DOLNGSPH\",\t# Numeric\t8\tNatural Gas cost for space heating, main and secondary, in dollars, 2015\n\t\t\t\t\"DOLNGWTH\",\t# Numeric\t8\tNatural gas cost for water heating, main and secondary, in dollars, 2015\n\t\t\t\t\"DOLNGCOK\",\t# Numeric\t8\tNatural gas cost for cooking (stoves, cooktops, and ovens), in dollars, 2015\n\t\t\t\t\"DOLNGCDR\",\t# Numeric\t8\tNatural gas cost for clothes dryers, in dollars, 2015\n\t\t\t\t\"DOLNGPLHEAT\",\t# Numeric\t8\tNatural gas cost for swimming pool heaters, in dollars, 2015\n\t\t\t\t\"DOLNGHTBHEAT\",\t# Numeric\t8\tNatural gas cost for hot tub heaters, in dollars, 2015\n\t\t\t\t\"DOLNGNEC\",\t# Numeric\t8\tNatural gas cost for other devices and purposes not elsewhere classified, in dollars, 2015\n\t\t\t\t\"GALLONLP\",\t# Numeric\t8\tTotal propane usage, in gallons, 2015\n\t\t\t\t\"GALLONLPSPH\",\t# Numeric\t8\tPropane usage for space heating, main and secondary, in gallons, 2015\n\t\t\t\t\"GALLONLPWTH\",\t# Numeric\t8\tPropane usage for water heating, main and secondary, in gallons, 2015\n\t\t\t\t\"GALLONLPCOK\",\t# Numeric\t8\tPropane usage for cooking (stoves, cooktops, and ovens), in gallons, 2015\n\t\t\t\t\"GALLONLPCDR\",\t# Numeric\t8\tPropane usage for clothes dryers, in gallons, 2015\n\t\t\t\t\"GALLONLPNEC\",\t# Numeric\t8\tPropane usage for other devices and purposes not elsewhere classified, in gallons, 2015\n\t\t\t\t\"BTULP\",\t# Numeric\t8\tTotal propane usage, in thousand Btu, 2015\n\t\t\t\t\"BTULPSPH\",\t# Numeric\t8\tPropane usage for space heating, main and secondary, in thousand Btu, 2015\n\t\t\t\t\"BTULPWTH\",\t# Numeric\t8\tPropane usage for water heating, main and secondary, in thousand Btu, 2015\n\t\t\t\t\"BTULPCOK\",\t# Numeric\t8\tPropane usage for cooking (stoves, cooktops, and ovens), in thousand Btu, 2015\n\t\t\t\t\"BTULPCDR\",\t# Numeric\t8\tPropane usage for clothes dryers, in thousand Btu, 2015\n\t\t\t\t\"BTULPNEC\",\t# Numeric\t8\tPropane usage for other devices and purposes not elsewhere classified, in thousand Btu, 2015\n\t\t\t\t\"DOLLARLP\",\t# Numeric\t8\tTotal cost of propane, in dollars, 2015\n\t\t\t\t\"DOLLPSPH\",\t# Numeric\t8\tPropane cost for space heating, main and secondary, in dollars, 2015\n\t\t\t\t\"DOLLPWTH\",\t# Numeric\t8\tPropane cost for water heating, main and secondary, in dollars, 2015\n\t\t\t\t\"DOLLPCOK\",\t# Numeric\t8\tPropane cost for cooking (stoves, cooktops, and ovens), in dollars, 2015\n\t\t\t\t\"DOLLPCDR\",\t# Numeric\t8\tPropane cost for clothes dryers, in dollars, 2015\n\t\t\t\t\"DOLLPNEC\",\t# Numeric\t8\tPropane cost for other devices and purposes not elsewhere classified, in dollars, 2015\n\t\t\t\t\"GALLONFO\",\t# Numeric\t8\tTotal fuel oil/kerosene usage, in gallons, 2015\n\t\t\t\t\"GALLONFOSPH\",\t# Numeric\t8\tFuel oil/kerosene usage for space heating, main and secondary, in gallons, 2015\n\t\t\t\t\"GALLONFOWTH\",\t# Numeric\t8\tFuel oil/kerosene usage for water heating, main and secondary, in gallons, 2015\n\t\t\t\t\"GALLONFONEC\",\t# Numeric\t8\tFuel oil/kerosene usage for other devices and purposes not elsewhere classified, in gallons, 2015\n\t\t\t\t\"BTUFO\",\t# Numeric\t8\tTotal fuel oil/kerosene usage, in thousand Btu, 2015\n\t\t\t\t\"BTUFOSPH\",\t# Numeric\t8\tFuel oil/kerosene usage for space heating, main and secondary, in thousand Btu, 2015\n\t\t\t\t\"BTUFOWTH\",\t# Numeric\t8\tFuel oil/kerosene usage for water heating, main and secondary, in thousand Btu, 2015\n\t\t\t\t\"BTUFONEC\",\t# Numeric\t8\tFuel oil/kerosene usage for other devices and purposes not elsewhere classified, in thousand Btu, 2015\n\t\t\t\t\"DOLLARFO\",\t# Numeric\t8\tTotal cost of fuel oil/kerosene, in dollars, 2015\n\t\t\t\t\"DOLFOSPH\",\t# Numeric\t8\tFuel oil/kerosene cost for space heating, main and secondary, in dollars, 2015\n\t\t\t\t\"DOLFOWTH\",\t# Numeric\t8\tFuel oil/kerosene cost for water heating, main and secondary, in dollars, 2015\n\t\t\t\t\"DOLFONEC\",\t# Numeric\t8\tFuel oil/kerosene cost for other devices and purposes not elsewhere classified, in dollars, 2015\n\t\t\t\t\"TOTALBTU\",\t# Numeric\t8\tTotal usage, in thousand Btu, 2015\n\t\t\t\t\"TOTALDOL\",\t# Numeric\t8\tTotal cost, in dollars, 2015 \n\t\t\t\t\"TOTALBTUSPH\",\t# Numeric\t8\tTotal usage for space heating, main and secondary, in thousand Btu, 2015\n\t\t\t\t\"TOTALDOLSPH\",\t# Numeric\t8\tTotal cost for space heating, main and secondary, in dollars, 2015\n\t\t\t\t\"TOTALBTUWTH\",\t# Numeric\t8\tTotal usage for water heating, main and secondary, in thousand Btu, 2015\n\t\t\t\t\"TOTALDOLWTH\",\t# Numeric\t8\tTotal cost for water heating, main and secondary, in dollars, 2015\n\t\t\t\t\"TOTALBTUCOK\",\t# Numeric\t8\tTotal usage for cooking (stoves, cooktops, and ovens), in thousand Btu, 2015\n\t\t\t\t\"TOTALDOLCOK\",\t# Numeric\t8\tTotal cost for cooking (stoves, cooktops, and ovens), in dollars, 2015\n\t\t\t\t\"TOTALBTUCDR\",\t# Numeric\t8\tTotal usage for clothes dryers, in thousand Btu, 2015\n\t\t\t\t\"TOTALDOLCDR\",\t# Numeric\t8\tTotal cost for clothes dryers, in dollars, 2015\n\t\t\t\t\"TOTALBTUPL\",\t# Numeric\t8\tTotal usage for swimming pool pumps and heaters, in thousand Btu, 2015\n\t\t\t\t\"TOTALDOLPL\",\t# Numeric\t8\tTotal cost for swimming pool pumps and heaters, in dollars, 2015\n\t\t\t\t\"TOTALBTUHTB\",\t# Numeric\t8\tTotal usage for hot tub pumps and heaters, in thousand Btu, 2015\n\t\t\t\t\"TOTALDOLHTB\",\t# Numeric\t8\tTotal cost for hot tub pumps and heaters, in dollars, 2015\n\t\t\t\t\"TOTALBTUNEC\",\t# Numeric\t8\tTotal usage for other devices and purposes not elsewhere classified, in thousand Btu, 2015\n\t\t\t\t\"TOTALDOLNEC\",\t# Numeric\t8\tTotal cost for other devices and purposes not elsewhere classified, in thousand Btu, 2015\n\t\t\t\t\"WOODAMT\",\t# Numeric\t8\tCords of wood used in the last year\n\t\t\t\t\"ZWOODAMT\",\t# Numeric\t8\tImputation flag for WOODAMT\n\t\t\t\t\"WOODBTU\",\t# Numeric\t8\tTotal cordwood usage, in thousand Btu, 2015 (Wood consumption is not included in TOTALBTU or TOTALDOL)\n\t\t\t\t\"PELLETAMT\",\t# Numeric\t8\tNumber of 40-pound wood pellet bags used in the last year\n\t\t\t\t\"ZPELLETAMT\",\t# Numeric\t8\tImputation flag for PELLETAMT\n\t\t\t\t\"PELLETBTU\"\t# Numeric\t8\tTotal wood pellet usage, in thousand Btu, 2015 (Wood consumption is not included in TOTALBTU or TOTALDOL)\n]\n\n# Log-scale all of our targets\nfor target in potential_targets[:]:\n    # We use log(3+n) for simplicity's sake\n    # as Not Applicable is encoded as -2 in the dataset\n\trecs[target+'_LOG'] = numpy.log(3+recs[target])\n\tnot_features.append(target+'_LOG')\n\tpotential_targets.append(target+'_LOG')\n\n# The features we'll be using for our predictions\n# These are the most important features for our various targets\n# features = recs.columns.drop(not_features)\nfeatures = ['TOTSQFT_EN',\n\t\t\t'TOTCSQFT',\n\t\t\t'TOTHSQFT',\n\t\t\t'FUELHEAT',\n\t\t\t'REGIONC',\n\t\t\t'NHSLDMEM',\n\t\t\t'TOTROOMS',\n\t\t\t'WASHLOAD',\n\t\t\t'NUMADULT',\n\t\t\t'FUELH2O',\n\t\t\t'DRYRUSE',\n\t\t\t'CLIMATE_REGION_PUB',\n\t\t\t'DBT99',\n\t\t\t'FUELAUX']\nweights = ['NWEIGHT']\n# targets = ['btu_log']\n# features\n"},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TOTSQFT_EN</th>\n      <th>TOTCSQFT</th>\n      <th>TOTHSQFT</th>\n      <th>FUELHEAT</th>\n      <th>REGIONC</th>\n      <th>NHSLDMEM</th>\n      <th>TOTROOMS</th>\n      <th>WASHLOAD</th>\n      <th>NUMADULT</th>\n      <th>FUELH2O</th>\n      <th>DRYRUSE</th>\n      <th>DBT99</th>\n      <th>FUELAUX</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5686.000000</td>\n      <td>5686.000000</td>\n      <td>5686.000000</td>\n      <td>5686.000000</td>\n      <td>5686.000000</td>\n      <td>5686.000000</td>\n      <td>5686.000000</td>\n      <td>5686.000000</td>\n      <td>5686.000000</td>\n      <td>5686.00000</td>\n      <td>5686.00000</td>\n      <td>5686.000000</td>\n      <td>5686.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2081.443546</td>\n      <td>1454.523391</td>\n      <td>1815.814281</td>\n      <td>2.590046</td>\n      <td>2.760816</td>\n      <td>2.577383</td>\n      <td>6.191347</td>\n      <td>3.609743</td>\n      <td>1.972212</td>\n      <td>2.93739</td>\n      <td>3.48892</td>\n      <td>20.361115</td>\n      <td>0.459022</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1282.660286</td>\n      <td>1253.846163</td>\n      <td>1178.533895</td>\n      <td>2.302666</td>\n      <td>1.004187</td>\n      <td>1.432224</td>\n      <td>2.360918</td>\n      <td>4.019369</td>\n      <td>0.855589</td>\n      <td>1.98799</td>\n      <td>4.09300</td>\n      <td>14.748237</td>\n      <td>3.404171</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>221.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>-2.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>-2.00000</td>\n      <td>-19.600000</td>\n      <td>-2.000000</td>\n    </tr>\n    <tr>\n      <th>2.5%</th>\n      <td>544.250000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>-2.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>-2.00000</td>\n      <td>-8.475000</td>\n      <td>-2.000000</td>\n    </tr>\n    <tr>\n      <th>5%</th>\n      <td>640.000000</td>\n      <td>0.000000</td>\n      <td>385.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>-2.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>-2.00000</td>\n      <td>-1.800000</td>\n      <td>-2.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1773.500000</td>\n      <td>1218.500000</td>\n      <td>1559.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>6.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>2.00000</td>\n      <td>3.00000</td>\n      <td>19.100000</td>\n      <td>-2.000000</td>\n    </tr>\n    <tr>\n      <th>95%</th>\n      <td>4504.000000</td>\n      <td>3850.000000</td>\n      <td>4072.000000</td>\n      <td>5.000000</td>\n      <td>4.000000</td>\n      <td>5.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>4.000000</td>\n      <td>5.00000</td>\n      <td>10.00000</td>\n      <td>42.900000</td>\n      <td>7.000000</td>\n    </tr>\n    <tr>\n      <th>97.5%</th>\n      <td>5366.875000</td>\n      <td>4415.750000</td>\n      <td>4820.000000</td>\n      <td>7.000000</td>\n      <td>4.000000</td>\n      <td>6.000000</td>\n      <td>11.000000</td>\n      <td>14.000000</td>\n      <td>4.000000</td>\n      <td>5.00000</td>\n      <td>14.00000</td>\n      <td>46.000000</td>\n      <td>7.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>8501.000000</td>\n      <td>8066.000000</td>\n      <td>8066.000000</td>\n      <td>21.000000</td>\n      <td>4.000000</td>\n      <td>12.000000</td>\n      <td>19.000000</td>\n      <td>30.000000</td>\n      <td>10.000000</td>\n      <td>21.00000</td>\n      <td>30.00000</td>\n      <td>63.900000</td>\n      <td>21.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"        TOTSQFT_EN     TOTCSQFT     TOTHSQFT     FUELHEAT      REGIONC  \\\ncount  5686.000000  5686.000000  5686.000000  5686.000000  5686.000000   \nmean   2081.443546  1454.523391  1815.814281     2.590046     2.760816   \nstd    1282.660286  1253.846163  1178.533895     2.302666     1.004187   \nmin     221.000000     0.000000     0.000000    -2.000000     1.000000   \n2.5%    544.250000     0.000000     0.000000    -2.000000     1.000000   \n5%      640.000000     0.000000   385.000000     1.000000     1.000000   \n50%    1773.500000  1218.500000  1559.000000     1.000000     3.000000   \n95%    4504.000000  3850.000000  4072.000000     5.000000     4.000000   \n97.5%  5366.875000  4415.750000  4820.000000     7.000000     4.000000   \nmax    8501.000000  8066.000000  8066.000000    21.000000     4.000000   \n\n          NHSLDMEM     TOTROOMS     WASHLOAD     NUMADULT     FUELH2O  \\\ncount  5686.000000  5686.000000  5686.000000  5686.000000  5686.00000   \nmean      2.577383     6.191347     3.609743     1.972212     2.93739   \nstd       1.432224     2.360918     4.019369     0.855589     1.98799   \nmin       1.000000     1.000000    -2.000000     1.000000     1.00000   \n2.5%      1.000000     2.000000    -2.000000     1.000000     1.00000   \n5%        1.000000     3.000000    -2.000000     1.000000     1.00000   \n50%       2.000000     6.000000     3.000000     2.000000     2.00000   \n95%       5.000000    10.000000    10.000000     4.000000     5.00000   \n97.5%     6.000000    11.000000    14.000000     4.000000     5.00000   \nmax      12.000000    19.000000    30.000000    10.000000    21.00000   \n\n          DRYRUSE        DBT99      FUELAUX  \ncount  5686.00000  5686.000000  5686.000000  \nmean      3.48892    20.361115     0.459022  \nstd       4.09300    14.748237     3.404171  \nmin      -2.00000   -19.600000    -2.000000  \n2.5%     -2.00000    -8.475000    -2.000000  \n5%       -2.00000    -1.800000    -2.000000  \n50%       3.00000    19.100000    -2.000000  \n95%      10.00000    42.900000     7.000000  \n97.5%    14.00000    46.000000     7.000000  \nmax      30.00000    63.900000    21.000000  "},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":"# Get some info on the dataset, for scaling input sliders\nrecs[features].describe(percentiles=[0.025, 0.05,0.5,0.95, 0.975])"},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CLIMATE_REGION_PUB</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5686</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Cold/Very Cold</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>2008</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       CLIMATE_REGION_PUB\ncount                5686\nunique                  5\ntop        Cold/Very Cold\nfreq                 2008"},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":"# And info for defaults for non-numeric dropdowns\nrecs[features].describe(exclude=numpy.number)"},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"data":{"text/plain":"['TOTALBTU',\n 'TOTALDOL',\n 'KWH',\n 'BTUEL',\n 'DOLLAREL',\n 'CUFEETNG',\n 'BTUNG',\n 'DOLLARNG',\n 'GALLONLP',\n 'BTULP',\n 'DOLLARLP',\n 'GALLONFO',\n 'BTUFO',\n 'DOLLARFO',\n 'WOODAMT',\n 'WOODBTU',\n 'TOTALBTU_LOG',\n 'TOTALDOL_LOG',\n 'KWH_LOG',\n 'BTUEL_LOG',\n 'DOLLAREL_LOG',\n 'CUFEETNG_LOG',\n 'BTUNG_LOG',\n 'DOLLARNG_LOG',\n 'GALLONLP_LOG',\n 'BTULP_LOG',\n 'DOLLARLP_LOG',\n 'GALLONFO_LOG',\n 'BTUFO_LOG',\n 'DOLLARFO_LOG',\n 'WOODAMT_LOG',\n 'WOODBTU_LOG']"},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":"# Our full list of targets\npotential_targets"},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":"# We'll be one-hot encoding each column with <= 11 unique values\nuniques = recs[features].nunique()\nencode = list(uniques[uniques<=11].index)"},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":"from category_encoders import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n\noneHot = OneHotEncoder(use_cat_names=True, cols=encode)\n\n# features_transformed = oneHot.fit_transform(recs[features])\n"},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":"# Due to the limited hyperparameter tuning and data available, we do a two-way split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val, w_train, w_val = train_test_split(recs[features].sort_index(axis=1), recs[potential_targets], recs[weights], random_state=3)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"A multioutput linear regression model gets R^2 of 0.63"},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"/opt/anaconda/anaconda3/lib/python3.7/site-packages/sklearn/base.py:420: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n  \"multioutput='uniform_average').\", FutureWarning)\n"},{"data":{"text/plain":"0.6291696692851972"},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":"from sklearn.linear_model import LinearRegression\n\n_lr = LinearRegression()\n\nlr_pipeline = Pipeline([('OneHotEncoder', oneHot),\n                        ('LinearRegression', _lr)])\n\nlr_pipeline.fit(X_train, y_train)\n\nlr_pipeline.score(X_val, y_val)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"And individual target scores:"},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Training on TOTALBTU\nTOTALBTU score: 0.581088418882282\nTraining on TOTALDOL\nTOTALDOL score: 0.5104241832316625\nTraining on KWH\nKWH score: 0.5058279180679004\nTraining on BTUEL\nBTUEL score: 0.5058278711604289\nTraining on DOLLAREL\nDOLLAREL score: 0.43644164638738175\nTraining on CUFEETNG\nCUFEETNG score: 0.6698378846585337\nTraining on BTUNG\nBTUNG score: 0.6695750656497823\nTraining on DOLLARNG\nDOLLARNG score: 0.685096654889336\nTraining on GALLONLP\nGALLONLP score: 0.8041021376138633\nTraining on BTULP\nBTULP score: 0.8041020665360166\nTraining on DOLLARLP\nDOLLARLP score: 0.7820852115956208\nTraining on GALLONFO\nGALLONFO score: 0.829222037442764\nTraining on BTUFO\nBTUFO score: 0.8287814949697866\nTraining on DOLLARFO\nDOLLARFO score: 0.823105415856515\nTraining on WOODAMT\nWOODAMT score: 0.8746927841362786\nTraining on WOODBTU\nWOODBTU score: 0.6424448095752731\nTraining on TOTALBTU_LOG\nTOTALBTU_LOG score: 0.6282491132474428\nTraining on TOTALDOL_LOG\nTOTALDOL_LOG score: 0.5289751167746957\nTraining on KWH_LOG\nKWH_LOG score: 0.5511312044874026\nTraining on BTUEL_LOG\nBTUEL_LOG score: 0.5510595745379746\nTraining on DOLLAREL_LOG\nDOLLAREL_LOG score: 0.45689855611197683\nTraining on CUFEETNG_LOG\nCUFEETNG_LOG score: 0.8799715863872343\nTraining on BTUNG_LOG\nBTUNG_LOG score: 0.867037946785508\nTraining on DOLLARNG_LOG\nDOLLARNG_LOG score: 0.8704228604261602\nTraining on GALLONLP_LOG\nGALLONLP_LOG score: 0.8281246056067895\nTraining on BTULP_LOG\nBTULP_LOG score: 0.7691683286610951\nTraining on DOLLARLP_LOG\nDOLLARLP_LOG score: 0.7957367493523135\nTraining on GALLONFO_LOG\nGALLONFO_LOG score: 0.9584465174161981\nTraining on BTUFO_LOG\nBTUFO_LOG score: 0.9661999501648529\nTraining on DOLLARFO_LOG\nDOLLARFO_LOG score: 0.961977896371719\nTraining on WOODAMT_LOG\nWOODAMT_LOG score: 0.9762016830086958\nTraining on WOODBTU_LOG\nWOODBTU_LOG score: 0.9482358878476166\n"}],"source":"from sklearn.linear_model import LinearRegression\n\n_lr = LinearRegression()\n\nlr_pipeline = Pipeline([('OneHotEncoder', oneHot),\n                        ('LinearRegression', _lr)])\nfor i in range(len(potential_targets)):\n    target = potential_targets[i]\n    print('Training on', target)\n    y_train_t = y_train.iloc[:,i]\n    y_val_t = y_val.iloc[:,i]\n    lr_pipeline.fit(X_train, y_train_t)\n\n    print(target, 'score:', lr_pipeline.score(X_val, y_val_t))"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"A multioutput RFR shows some improvement"},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"best_score_:  0.6525034240493588\nbest_estimator_.score:  0.6602883575740279\n/opt/anaconda/anaconda3/lib/python3.7/site-packages/sklearn/base.py:420: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n  \"multioutput='uniform_average').\", FutureWarning)\n"}],"source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrfr_model = RandomForestRegressor(n_estimators=170, min_samples_leaf=12, n_jobs=-1, verbose=0)\n\npipeline = Pipeline([('OneHotEncoder', oneHot),\n                    ('RandomForestRegressor', rfr_model)])\n\nsearchCV = RandomizedSearchCV(\tpipeline,\n\t\t\t\t\t\t\t\tparam_distributions={},\n\t\t\t\t\t\t\t\tn_iter=1,\n\t\t\t\t\t\t\t\tcv=9,\n\t\t\t\t\t\t\t\tscoring='r2',\n\t\t\t\t\t\t\t\tverbose=0,\n\t\t\t\t\t\t\t\treturn_train_score=True,\n\t\t\t\t\t\t\t\tn_jobs=-1)\n\nsearchCV.fit(X_train, y_train)\n\nprint('best_score_: ', searchCV.best_score_)#.score(X_val, y_val)\nprint('best_estimator_.score: ', searchCV.best_estimator_.score(X_val, y_val))"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"Individual output predictions"},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"TOTALBTU score: 0.6009056228636007\nTOTALDOL score: 0.47213855116836856\nKWH score: 0.4899967891146956\nBTUEL score: 0.4899969524537454\nDOLLAREL score: 0.41515272831598415\nCUFEETNG score: 0.7478671484686906\nBTUNG score: 0.7439516028916531\nDOLLARNG score: 0.7296682158962877\nGALLONLP score: 0.742850926500848\nBTULP score: 0.7428509315968315\nDOLLARLP score: 0.6873702717516195\nGALLONFO score: 0.8797776469062608\nBTUFO score: 0.879525830718417\nDOLLARFO score: 0.869804749697185\nWOODAMT score: 0.5496895160984494\nWOODBTU score: 0.6235845679194743\nTOTALBTU_LOG score: 0.6506439872014367\nTOTALDOL_LOG score: 0.5144608697279833\nKWH_LOG score: 0.530429934475584\nBTUEL_LOG score: 0.5303626240050128\nDOLLAREL_LOG score: 0.4441253003613952\nCUFEETNG_LOG score: 0.9161539839702866\nBTUNG_LOG score: 0.9092959578937245\nDOLLARNG_LOG score: 0.9083544983734795\nGALLONLP_LOG score: 0.7136805974955145\nBTULP_LOG score: 0.6594913385632899\nDOLLARLP_LOG score: 0.684781534428611\nGALLONFO_LOG score: 0.8939613228435527\nBTUFO_LOG score: 0.8580208388254356\nDOLLARFO_LOG score: 0.8859134606800376\nWOODAMT_LOG score: 0.45300248224329487\nWOODBTU_LOG score: 0.3964203185971472\n"}],"source":"from sklearn.metrics import r2_score\n\ny_pred = searchCV.best_estimator_.predict(X_val)\nfor i in range(len(potential_targets)):\n\ttarget = potential_targets[i]\n\ty_val_t = y_val.iloc[:,i]\n\ty_pred_t = y_pred[:,i]\n\tprint(target, 'score:', r2_score(y_val_t, y_pred_t))"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"#### Training on individual targets, then save the pipelines ####"},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Training on TOTALBTU\nTOTALBTU score: 0.5995394059869292\nTraining on TOTALDOL\nTOTALDOL score: 0.5181794532749524\nTraining on KWH\nKWH score: 0.5250271328584624\nTraining on BTUEL\nBTUEL score: 0.5235438767966549\nTraining on DOLLAREL\nDOLLAREL score: 0.4494952620156775\nTraining on CUFEETNG\nCUFEETNG score: 0.7616862715540488\nTraining on BTUNG\nBTUNG score: 0.7627776686813691\nTraining on DOLLARNG\nDOLLARNG score: 0.774710953426889\nTraining on GALLONLP\nGALLONLP score: 0.8232821684047951\nTraining on BTULP\nBTULP score: 0.8239573010995185\nTraining on DOLLARLP\nDOLLARLP score: 0.7862896796773782\nTraining on GALLONFO\nGALLONFO score: 0.8937446955559482\nTraining on BTUFO\nBTUFO score: 0.8948328534302867\nTraining on DOLLARFO\nDOLLARFO score: 0.8853943705775262\nTraining on WOODAMT\nWOODAMT score: 0.9078965558443368\nTraining on WOODBTU\nWOODBTU score: 0.755085643647531\nTraining on TOTALBTU_LOG\nTOTALBTU_LOG score: 0.6428810344669216\nTraining on TOTALDOL_LOG\nTOTALDOL_LOG score: 0.5472195297346523\nTraining on KWH_LOG\nKWH_LOG score: 0.5604479953737093\nTraining on BTUEL_LOG\nBTUEL_LOG score: 0.5616464498770566\nTraining on DOLLAREL_LOG\nDOLLAREL_LOG score: 0.4842614604261163\nTraining on CUFEETNG_LOG\nCUFEETNG_LOG score: 0.9621556544244695\nTraining on BTUNG_LOG\nBTUNG_LOG score: 0.9616146903482099\nTraining on DOLLARNG_LOG\nDOLLARNG_LOG score: 0.9580735743476151\nTraining on GALLONLP_LOG\nGALLONLP_LOG score: 0.9046638174250635\nTraining on BTULP_LOG\nBTULP_LOG score: 0.8660852234080733\nTraining on DOLLARLP_LOG\nDOLLARLP_LOG score: 0.8842246852723505\nTraining on GALLONFO_LOG\nGALLONFO_LOG score: 0.9606233923017046\nTraining on BTUFO_LOG\nBTUFO_LOG score: 0.9712165086577573\nTraining on DOLLARFO_LOG\nDOLLARFO_LOG score: 0.9642252018175955\nTraining on WOODAMT_LOG\nWOODAMT_LOG score: 0.9812551734769057\nTraining on WOODBTU_LOG\nWOODBTU_LOG score: 0.9539635304321814\n"}],"source":"import joblib\n\nmodels = {}\n\nfor i in range(len(potential_targets)):\n    target = potential_targets[i]\n    print('Training on', target)\n    y_train_t = y_train.iloc[:,i]\n    y_val_t = y_val.iloc[:,i]\n\n    #n_estimators=120\n    rfr_model_t = RandomForestRegressor(n_estimators=270, min_samples_leaf=12, n_jobs=-2, verbose=0)\n\n    pipeline_t = Pipeline([ ('OneHotEncoder', oneHot),\n                            ('RandomForestRegressor', rfr_model)])\n\n    searchCV = RandomizedSearchCV(\tpipeline,\n                                    param_distributions={},\n                                    n_iter=1,\n                                    cv=3,\n                                    scoring='r2',\n                                    verbose=0,\n                                    return_train_score=True,\n                                    n_jobs=-2)\n\n    searchCV.fit(X_train, y_train_t)\n    searchCV.best_estimator_.verbose=0\n\n    models[target] = searchCV.best_estimator_\n    print(target, 'score:', models[target].score(X_val, y_val_t))\n\n    joblib.dump(models[target], './Project-2/pipelines/'+target+'_pipeline.joblib', compress=True)\n\n"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"#### Train an XGBRegressor model on each target ####\nNote that we do not use these models in the webapp, due to memory constraints"},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":".696511\tvalidation_1-rmse:0.617724\n[197]\tvalidation_0-rmse:0.695525\tvalidation_1-rmse:0.617501\n[198]\tvalidation_0-rmse:0.695476\tvalidation_1-rmse:0.617498\n[199]\tvalidation_0-rmse:0.695429\tvalidation_1-rmse:0.617493\n[200]\tvalidation_0-rmse:0.695383\tvalidation_1-rmse:0.617464\n[201]\tvalidation_0-rmse:0.694431\tvalidation_1-rmse:0.61727\n[202]\tvalidation_0-rmse:0.693819\tvalidation_1-rmse:0.617125\n[203]\tvalidation_0-rmse:0.693776\tvalidation_1-rmse:0.617126\n[204]\tvalidation_0-rmse:0.693223\tvalidation_1-rmse:0.617061\n[205]\tvalidation_0-rmse:0.693182\tvalidation_1-rmse:0.617063\n[206]\tvalidation_0-rmse:0.693143\tvalidation_1-rmse:0.617066\n[207]\tvalidation_0-rmse:0.692614\tvalidation_1-rmse:0.617014\n[208]\tvalidation_0-rmse:0.692576\tvalidation_1-rmse:0.616992\n[209]\tvalidation_0-rmse:0.692281\tvalidation_1-rmse:0.616915\n[210]\tvalidation_0-rmse:0.692244\tvalidation_1-rmse:0.61692\n[211]\tvalidation_0-rmse:0.692056\tvalidation_1-rmse:0.616874\n[212]\tvalidation_0-rmse:0.692021\tvalidation_1-rmse:0.616889\n[213]\tvalidation_0-rmse:0.691516\tvalidation_1-rmse:0.6172\n[214]\tvalidation_0-rmse:0.691238\tvalidation_1-rmse:0.617085\n[215]\tvalidation_0-rmse:0.691205\tvalidation_1-rmse:0.617091\n[216]\tvalidation_0-rmse:0.690775\tvalidation_1-rmse:0.617169\n[217]\tvalidation_0-rmse:0.690645\tvalidation_1-rmse:0.617047\n[218]\tvalidation_0-rmse:0.690472\tvalidation_1-rmse:0.617048\n[219]\tvalidation_0-rmse:0.690057\tvalidation_1-rmse:0.617131\n[220]\tvalidation_0-rmse:0.689933\tvalidation_1-rmse:0.617018\n[221]\tvalidation_0-rmse:0.689903\tvalidation_1-rmse:0.617039\n[222]\tvalidation_0-rmse:0.689419\tvalidation_1-rmse:0.617118\n[223]\tvalidation_0-rmse:0.689301\tvalidation_1-rmse:0.61701\n[224]\tvalidation_0-rmse:0.689264\tvalidation_1-rmse:0.616989\n[225]\tvalidation_0-rmse:0.688794\tvalidation_1-rmse:0.617068\n[226]\tvalidation_0-rmse:0.688174\tvalidation_1-rmse:0.61709\n[227]\tvalidation_0-rmse:0.687776\tvalidation_1-rmse:0.617176\n[228]\tvalidation_0-rmse:0.687748\tvalidation_1-rmse:0.617182\n[229]\tvalidation_0-rmse:0.687377\tvalidation_1-rmse:0.61719\n[230]\tvalidation_0-rmse:0.687342\tvalidation_1-rmse:0.617171\n[231]\tvalidation_0-rmse:0.686745\tvalidation_1-rmse:0.61721\n[232]\tvalidation_0-rmse:0.68639\tvalidation_1-rmse:0.617237\n[233]\tvalidation_0-rmse:0.686224\tvalidation_1-rmse:0.617256\n[234]\tvalidation_0-rmse:0.686105\tvalidation_1-rmse:0.617221\n[235]\tvalidation_0-rmse:0.685987\tvalidation_1-rmse:0.617194\n[236]\tvalidation_0-rmse:0.685863\tvalidation_1-rmse:0.617177\n[237]\tvalidation_0-rmse:0.685704\tvalidation_1-rmse:0.617207\n[238]\tvalidation_0-rmse:0.685677\tvalidation_1-rmse:0.617231\n[239]\tvalidation_0-rmse:0.685371\tvalidation_1-rmse:0.617167\n[240]\tvalidation_0-rmse:0.685339\tvalidation_1-rmse:0.617149\n[241]\tvalidation_0-rmse:0.684765\tvalidation_1-rmse:0.617211\n[242]\tvalidation_0-rmse:0.684739\tvalidation_1-rmse:0.617237\n[243]\tvalidation_0-rmse:0.684585\tvalidation_1-rmse:0.617266\n[244]\tvalidation_0-rmse:0.684561\tvalidation_1-rmse:0.617283\n[245]\tvalidation_0-rmse:0.684364\tvalidation_1-rmse:0.617201\n[246]\tvalidation_0-rmse:0.684024\tvalidation_1-rmse:0.617122\n[247]\tvalidation_0-rmse:0.6836\tvalidation_1-rmse:0.617008\n[248]\tvalidation_0-rmse:0.683576\tvalidation_1-rmse:0.617014\n[249]\tvalidation_0-rmse:0.683427\tvalidation_1-rmse:0.617034\n[250]\tvalidation_0-rmse:0.683237\tvalidation_1-rmse:0.616958\n[251]\tvalidation_0-rmse:0.682863\tvalidation_1-rmse:0.617002\n[252]\tvalidation_0-rmse:0.682677\tvalidation_1-rmse:0.616928\n[253]\tvalidation_0-rmse:0.682505\tvalidation_1-rmse:0.616906\n[254]\tvalidation_0-rmse:0.682071\tvalidation_1-rmse:0.616839\n[255]\tvalidation_0-rmse:0.681927\tvalidation_1-rmse:0.616869\n[256]\tvalidation_0-rmse:0.681826\tvalidation_1-rmse:0.616856\n[257]\tvalidation_0-rmse:0.681523\tvalidation_1-rmse:0.616963\n[258]\tvalidation_0-rmse:0.681346\tvalidation_1-rmse:0.616893\n[259]\tvalidation_0-rmse:0.681321\tvalidation_1-rmse:0.616908\n[260]\tvalidation_0-rmse:0.681182\tvalidation_1-rmse:0.616929\n[261]\tvalidation_0-rmse:0.681011\tvalidation_1-rmse:0.616862\n[262]\tvalidation_0-rmse:0.680595\tvalidation_1-rmse:0.616834\n[263]\tvalidation_0-rmse:0.680431\tvalidation_1-rmse:0.616768\n[264]\tvalidation_0-rmse:0.680037\tvalidation_1-rmse:0.616666\n[265]\tvalidation_0-rmse:0.679938\tvalidation_1-rmse:0.61666\n[266]\tvalidation_0-rmse:0.679775\tvalidation_1-rmse:0.616641\n[267]\tvalidation_0-rmse:0.679616\tvalidation_1-rmse:0.61658\n[268]\tvalidation_0-rmse:0.679482\tvalidation_1-rmse:0.616609\n[269]\tvalidation_0-rmse:0.679076\tvalidation_1-rmse:0.616553\n[270]\tvalidation_0-rmse:0.678918\tvalidation_1-rmse:0.616536\n[271]\tvalidation_0-rmse:0.678526\tvalidation_1-rmse:0.616481\n[272]\tvalidation_0-rmse:0.678372\tvalidation_1-rmse:0.616422\n[273]\tvalidation_0-rmse:0.678101\tvalidation_1-rmse:0.61653\n[274]\tvalidation_0-rmse:0.678008\tvalidation_1-rmse:0.616527\n[275]\tvalidation_0-rmse:0.677855\tvalidation_1-rmse:0.616511\n[276]\tvalidation_0-rmse:0.677477\tvalidation_1-rmse:0.61646\n[277]\tvalidation_0-rmse:0.677331\tvalidation_1-rmse:0.616448\n[278]\tvalidation_0-rmse:0.676967\tvalidation_1-rmse:0.6164\n[279]\tvalidation_0-rmse:0.676887\tvalidation_1-rmse:0.616505\n[280]\tvalidation_0-rmse:0.676548\tvalidation_1-rmse:0.616419\n[281]\tvalidation_0-rmse:0.676106\tvalidation_1-rmse:0.616345\n[282]\tvalidation_0-rmse:0.675964\tvalidation_1-rmse:0.616335\n[283]\tvalidation_0-rmse:0.675815\tvalidation_1-rmse:0.616312\n[284]\tvalidation_0-rmse:0.675391\tvalidation_1-rmse:0.616221\n[285]\tvalidation_0-rmse:0.675261\tvalidation_1-rmse:0.616243\n[286]\tvalidation_0-rmse:0.67462\tvalidation_1-rmse:0.616195\n[287]\tvalidation_0-rmse:0.67428\tvalidation_1-rmse:0.616154\n[288]\tvalidation_0-rmse:0.673664\tvalidation_1-rmse:0.616112\n[289]\tvalidation_0-rmse:0.673528\tvalidation_1-rmse:0.616102\n[290]\tvalidation_0-rmse:0.673203\tvalidation_1-rmse:0.616061\n[291]\tvalidation_0-rmse:0.672778\tvalidation_1-rmse:0.615996\n[292]\tvalidation_0-rmse:0.672186\tvalidation_1-rmse:0.615958\n[293]\tvalidation_0-rmse:0.671954\tvalidation_1-rmse:0.615938\n[294]\tvalidation_0-rmse:0.671546\tvalidation_1-rmse:0.616015\n[295]\tvalidation_0-rmse:0.671193\tvalidation_1-rmse:0.616028\n[296]\tvalidation_0-rmse:0.670781\tvalidation_1-rmse:0.615969\n[297]\tvalidation_0-rmse:0.670649\tvalidation_1-rmse:0.615959\n[298]\tvalidation_0-rmse:0.67034\tvalidation_1-rmse:0.61592\n[299]\tvalidation_0-rmse:0.670196\tvalidation_1-rmse:0.6159\n[300]\tvalidation_0-rmse:0.669795\tvalidation_1-rmse:0.615832\n[301]\tvalidation_0-rmse:0.669668\tvalidation_1-rmse:0.615823\n[302]\tvalidation_0-rmse:0.66937\tvalidation_1-rmse:0.615787\n[303]\tvalidation_0-rmse:0.66923\tvalidation_1-rmse:0.615768\n[304]\tvalidation_0-rmse:0.668841\tvalidation_1-rmse:0.615717\n[305]\tvalidation_0-rmse:0.668169\tvalidation_1-rmse:0.615627\n[306]\tvalidation_0-rmse:0.667777\tvalidation_1-rmse:0.615704\n[307]\tvalidation_0-rmse:0.667642\tvalidation_1-rmse:0.615706\n[308]\tvalidation_0-rmse:0.667422\tvalidation_1-rmse:0.615694\n[309]\tvalidation_0-rmse:0.667082\tvalidation_1-rmse:0.615723\n[310]\tvalidation_0-rmse:0.666702\tvalidation_1-rmse:0.6158\n[311]\tvalidation_0-rmse:0.666417\tvalidation_1-rmse:0.615767\n[312]\tvalidation_0-rmse:0.666087\tvalidation_1-rmse:0.615833\n[313]\tvalidation_0-rmse:0.665963\tvalidation_1-rmse:0.615825\n[314]\tvalidation_0-rmse:0.665687\tvalidation_1-rmse:0.615794\n[315]\tvalidation_0-rmse:0.665556\tvalidation_1-rmse:0.615776\n[316]\tvalidation_0-rmse:0.665189\tvalidation_1-rmse:0.615835\n[317]\tvalidation_0-rmse:0.665068\tvalidation_1-rmse:0.615827\n[318]\tvalidation_0-rmse:0.664803\tvalidation_1-rmse:0.615799\n[319]\tvalidation_0-rmse:0.664596\tvalidation_1-rmse:0.615862\n[320]\tvalidation_0-rmse:0.664192\tvalidation_1-rmse:0.615517\n[321]\tvalidation_0-rmse:0.664065\tvalidation_1-rmse:0.6155\n[322]\tvalidation_0-rmse:0.663711\tvalidation_1-rmse:0.615559\n[323]\tvalidation_0-rmse:0.663391\tvalidation_1-rmse:0.615665\n[324]\tvalidation_0-rmse:0.663137\tvalidation_1-rmse:0.615639\n[325]\tvalidation_0-rmse:0.662761\tvalidation_1-rmse:0.61558\n[326]\tvalidation_0-rmse:0.662638\tvalidation_1-rmse:0.615583\n[327]\tvalidation_0-rmse:0.66252\tvalidation_1-rmse:0.615574\n[328]\tvalidation_0-rmse:0.662155\tvalidation_1-rmse:0.615532\n[329]\tvalidation_0-rmse:0.661794\tvalidation_1-rmse:0.615386\n[330]\tvalidation_0-rmse:0.66155\tvalidation_1-rmse:0.615363\n[331]\tvalidation_0-rmse:0.661435\tvalidation_1-rmse:0.615365\n[332]\tvalidation_0-rmse:0.661315\tvalidation_1-rmse:0.615368\n[333]\tvalidation_0-rmse:0.661123\tvalidation_1-rmse:0.615389\n[334]\tvalidation_0-rmse:0.660782\tvalidation_1-rmse:0.615464\n[335]\tvalidation_0-rmse:0.660471\tvalidation_1-rmse:0.615557\n[336]\tvalidation_0-rmse:0.660118\tvalidation_1-rmse:0.615504\n[337]\tvalidation_0-rmse:0.659785\tvalidation_1-rmse:0.615573\n[338]\tvalidation_0-rmse:0.659551\tvalidation_1-rmse:0.615552\n[339]\tvalidation_0-rmse:0.659209\tvalidation_1-rmse:0.615504\n[340]\tvalidation_0-rmse:0.658795\tvalidation_1-rmse:0.615446\n[341]\tvalidation_0-rmse:0.658606\tvalidation_1-rmse:0.615446\n[342]\tvalidation_0-rmse:0.65849\tvalidation_1-rmse:0.615429\n[343]\tvalidation_0-rmse:0.658168\tvalidation_1-rmse:0.615489\n[344]\tvalidation_0-rmse:0.657943\tvalidation_1-rmse:0.615458\n[345]\tvalidation_0-rmse:0.65783\tvalidation_1-rmse:0.615451\n[346]\tvalidation_0-rmse:0.657499\tvalidation_1-rmse:0.615418\n[347]\tvalidation_0-rmse:0.657317\tvalidation_1-rmse:0.615418\n[348]\tvalidation_0-rmse:0.657204\tvalidation_1-rmse:0.615421\n[349]\tvalidation_0-rmse:0.657024\tvalidation_1-rmse:0.615504\n[350]\tvalidation_0-rmse:0.656725\tvalidation_1-rmse:0.615609\n[351]\tvalidation_0-rmse:0.656411\tvalidation_1-rmse:0.615646\n[352]\tvalidation_0-rmse:0.656023\tvalidation_1-rmse:0.61531\n[353]\tvalidation_0-rmse:0.655732\tvalidation_1-rmse:0.615407\n[354]\tvalidation_0-rmse:0.655554\tvalidation_1-rmse:0.615409\n[355]\tvalidation_0-rmse:0.655445\tvalidation_1-rmse:0.615392\n[356]\tvalidation_0-rmse:0.655335\tvalidation_1-rmse:0.615385\n[357]\tvalidation_0-rmse:0.655119\tvalidation_1-rmse:0.615355\n[358]\tvalidation_0-rmse:0.654931\tvalidation_1-rmse:0.615358\n[359]\tvalidation_0-rmse:0.654759\tvalidation_1-rmse:0.615389\n[360]\tvalidation_0-rmse:0.654453\tvalidation_1-rmse:0.615458\n[361]\tvalidation_0-rmse:0.654171\tvalidation_1-rmse:0.615585\n[362]\tvalidation_0-rmse:0.653796\tvalidation_1-rmse:0.615258\n[363]\tvalidation_0-rmse:0.653498\tvalidation_1-rmse:0.615296\n[364]\tvalidation_0-rmse:0.653224\tvalidation_1-rmse:0.615458\n[365]\tvalidation_0-rmse:0.65305\tvalidation_1-rmse:0.61546\n[366]\tvalidation_0-rmse:0.652843\tvalidation_1-rmse:0.615432\n[367]\tvalidation_0-rmse:0.652738\tvalidation_1-rmse:0.615416\n[368]\tvalidation_0-rmse:0.652557\tvalidation_1-rmse:0.615401\n[369]\tvalidation_0-rmse:0.652449\tvalidation_1-rmse:0.615394\n[370]\tvalidation_0-rmse:0.65228\tvalidation_1-rmse:0.615396\n[371]\tvalidation_0-rmse:0.652079\tvalidation_1-rmse:0.615379\n[372]\tvalidation_0-rmse:0.651975\tvalidation_1-rmse:0.615373\n[373]\tvalidation_0-rmse:0.651684\tvalidation_1-rmse:0.61551\n[374]\tvalidation_0-rmse:0.651519\tvalidation_1-rmse:0.615512\n[375]\tvalidation_0-rmse:0.650892\tvalidation_1-rmse:0.615436\n[376]\tvalidation_0-rmse:0.650715\tvalidation_1-rmse:0.615437\n[377]\tvalidation_0-rmse:0.650428\tvalidation_1-rmse:0.615474\n[378]\tvalidation_0-rmse:0.650235\tvalidation_1-rmse:0.61546\n[379]\tvalidation_0-rmse:0.649875\tvalidation_1-rmse:0.615146\n[380]\tvalidation_0-rmse:0.649609\tvalidation_1-rmse:0.615293\n[381]\tvalidation_0-rmse:0.649436\tvalidation_1-rmse:0.615278\n[382]\tvalidation_0-rmse:0.649276\tvalidation_1-rmse:0.61528\n[383]\tvalidation_0-rmse:0.649173\tvalidation_1-rmse:0.615274\n[384]\tvalidation_0-rmse:0.648986\tvalidation_1-rmse:0.61525\n[385]\tvalidation_0-rmse:0.648381\tvalidation_1-rmse:0.615183\n[386]\tvalidation_0-rmse:0.648104\tvalidation_1-rmse:0.61522\n[387]\tvalidation_0-rmse:0.647756\tvalidation_1-rmse:0.614915\n[388]\tvalidation_0-rmse:0.647588\tvalidation_1-rmse:0.614914\n[389]\tvalidation_0-rmse:0.647329\tvalidation_1-rmse:0.615112\n[390]\tvalidation_0-rmse:0.647059\tvalidation_1-rmse:0.615181\n[391]\tvalidation_0-rmse:0.646878\tvalidation_1-rmse:0.615158\n[392]\tvalidation_0-rmse:0.646532\tvalidation_1-rmse:0.615121\n[393]\tvalidation_0-rmse:0.646354\tvalidation_1-rmse:0.615141\n[394]\tvalidation_0-rmse:0.64626\tvalidation_1-rmse:0.615126\n[395]\tvalidation_0-rmse:0.64616\tvalidation_1-rmse:0.61512\n[396]\tvalidation_0-rmse:0.645986\tvalidation_1-rmse:0.615099\n[397]\tvalidation_0-rmse:0.645824\tvalidation_1-rmse:0.615099\n[398]\tvalidation_0-rmse:0.645669\tvalidation_1-rmse:0.615102\n[399]\tvalidation_0-rmse:0.645283\tvalidation_1-rmse:0.615071\n[400]\tvalidation_0-rmse:0.644945\tvalidation_1-rmse:0.614773\n[401]\tvalidation_0-rmse:0.644794\tvalidation_1-rmse:0.614775\n[402]\tvalidation_0-rmse:0.644636\tvalidation_1-rmse:0.614778\n[403]\tvalidation_0-rmse:0.644296\tvalidation_1-rmse:0.614913\n[404]\tvalidation_0-rmse:0.644033\tvalidation_1-rmse:0.614951\n[405]\tvalidation_0-rmse:0.643706\tvalidation_1-rmse:0.61466\n[406]\tvalidation_0-rmse:0.643607\tvalidation_1-rmse:0.614654\n[407]\tvalidation_0-rmse:0.64344\tvalidation_1-rmse:0.614642\n[408]\tvalidation_0-rmse:0.643273\tvalidation_1-rmse:0.614665\n[409]\tvalidation_0-rmse:0.6432\tvalidation_1-rmse:0.614626\n[410]\tvalidation_0-rmse:0.64283\tvalidation_1-rmse:0.614598\n[411]\tvalidation_0-rmse:0.642511\tvalidation_1-rmse:0.614328\n[412]\tvalidation_0-rmse:0.642365\tvalidation_1-rmse:0.614331\n[413]\tvalidation_0-rmse:0.642212\tvalidation_1-rmse:0.614315\n[414]\tvalidation_0-rmse:0.64207\tvalidation_1-rmse:0.614318\n[415]\tvalidation_0-rmse:0.641747\tvalidation_1-rmse:0.614182\n[416]\tvalidation_0-rmse:0.641651\tvalidation_1-rmse:0.614177\n[417]\tvalidation_0-rmse:0.641493\tvalidation_1-rmse:0.614204\n[418]\tvalidation_0-rmse:0.641424\tvalidation_1-rmse:0.614161\n[419]\tvalidation_0-rmse:0.641069\tvalidation_1-rmse:0.614136\n[420]\tvalidation_0-rmse:0.640812\tvalidation_1-rmse:0.614169\n[421]\tvalidation_0-rmse:0.640503\tvalidation_1-rmse:0.613905\n[422]\tvalidation_0-rmse:0.640353\tvalidation_1-rmse:0.61389\n[423]\tvalidation_0-rmse:0.640216\tvalidation_1-rmse:0.613894\n[424]\tvalidation_0-rmse:0.639894\tvalidation_1-rmse:0.613994\n[425]\tvalidation_0-rmse:0.63976\tvalidation_1-rmse:0.613998\n[426]\tvalidation_0-rmse:0.639666\tvalidation_1-rmse:0.613992\n[427]\tvalidation_0-rmse:0.63952\tvalidation_1-rmse:0.613992\n[428]\tvalidation_0-rmse:0.639219\tvalidation_1-rmse:0.613723\n[429]\tvalidation_0-rmse:0.639127\tvalidation_1-rmse:0.613724\n[430]\tvalidation_0-rmse:0.638978\tvalidation_1-rmse:0.613756\n[431]\tvalidation_0-rmse:0.638912\tvalidation_1-rmse:0.613716\n[432]\tvalidation_0-rmse:0.638605\tvalidation_1-rmse:0.613588\n[433]\tvalidation_0-rmse:0.638452\tvalidation_1-rmse:0.613579\n[434]\tvalidation_0-rmse:0.638112\tvalidation_1-rmse:0.613535\n[435]\tvalidation_0-rmse:0.637994\tvalidation_1-rmse:0.613489\n[436]\tvalidation_0-rmse:0.637866\tvalidation_1-rmse:0.613492\n[437]\tvalidation_0-rmse:0.637776\tvalidation_1-rmse:0.613494\n[438]\tvalidation_0-rmse:0.637525\tvalidation_1-rmse:0.613526\n[439]\tvalidation_0-rmse:0.637443\tvalidation_1-rmse:0.613526\n[440]\tvalidation_0-rmse:0.637151\tvalidation_1-rmse:0.613275\n[441]\tvalidation_0-rmse:0.637011\tvalidation_1-rmse:0.61326\n[442]\tvalidation_0-rmse:0.63687\tvalidation_1-rmse:0.613297\n[443]\tvalidation_0-rmse:0.63681\tvalidation_1-rmse:0.613262\n[444]\tvalidation_0-rmse:0.636672\tvalidation_1-rmse:0.613315\n[445]\tvalidation_0-rmse:0.636631\tvalidation_1-rmse:0.61334\n[446]\tvalidation_0-rmse:0.636323\tvalidation_1-rmse:0.613372\n[447]\tvalidation_0-rmse:0.6362\tvalidation_1-rmse:0.613376\n[448]\tvalidation_0-rmse:0.636111\tvalidation_1-rmse:0.613371\n[449]\tvalidation_0-rmse:0.635964\tvalidation_1-rmse:0.613363\n[450]\tvalidation_0-rmse:0.635844\tvalidation_1-rmse:0.613367\n[451]\tvalidation_0-rmse:0.635758\tvalidation_1-rmse:0.613364\n[452]\tvalidation_0-rmse:0.635387\tvalidation_1-rmse:0.613503\n[453]\tvalidation_0-rmse:0.63492\tvalidation_1-rmse:0.613486\n[454]\tvalidation_0-rmse:0.634779\tvalidation_1-rmse:0.613479\n[455]\tvalidation_0-rmse:0.634694\tvalidation_1-rmse:0.613481\n[456]\tvalidation_0-rmse:0.634637\tvalidation_1-rmse:0.613447\n[457]\tvalidation_0-rmse:0.634454\tvalidation_1-rmse:0.613436\n[458]\tvalidation_0-rmse:0.634174\tvalidation_1-rmse:0.61319\n[459]\tvalidation_0-rmse:0.634037\tvalidation_1-rmse:0.61319\n[460]\tvalidation_0-rmse:0.633591\tvalidation_1-rmse:0.613189\n[461]\tvalidation_0-rmse:0.633479\tvalidation_1-rmse:0.613193\n[462]\tvalidation_0-rmse:0.632715\tvalidation_1-rmse:0.613107\n[463]\tvalidation_0-rmse:0.632416\tvalidation_1-rmse:0.613017\n[464]\tvalidation_0-rmse:0.632334\tvalidation_1-rmse:0.613012\n[465]\tvalidation_0-rmse:0.631978\tvalidation_1-rmse:0.613154\n[466]\tvalidation_0-rmse:0.631557\tvalidation_1-rmse:0.613167\n[467]\tvalidation_0-rmse:0.631422\tvalidation_1-rmse:0.613161\n[468]\tvalidation_0-rmse:0.631319\tvalidation_1-rmse:0.61312\n[469]\tvalidation_0-rmse:0.630776\tvalidation_1-rmse:0.613089\n[470]\tvalidation_0-rmse:0.63058\tvalidation_1-rmse:0.613089\n[471]\tvalidation_0-rmse:0.630255\tvalidation_1-rmse:0.613028\n[472]\tvalidation_0-rmse:0.630079\tvalidation_1-rmse:0.613021\n[473]\tvalidation_0-rmse:0.629551\tvalidation_1-rmse:0.613001\n[474]\tvalidation_0-rmse:0.62936\tvalidation_1-rmse:0.613038\n[475]\tvalidation_0-rmse:0.629053\tvalidation_1-rmse:0.613155\n[476]\tvalidation_0-rmse:0.628972\tvalidation_1-rmse:0.613161\n[477]\tvalidation_0-rmse:0.628236\tvalidation_1-rmse:0.613073\n[478]\tvalidation_0-rmse:0.627953\tvalidation_1-rmse:0.612963\n[479]\tvalidation_0-rmse:0.627773\tvalidation_1-rmse:0.612999\n[480]\tvalidation_0-rmse:0.627673\tvalidation_1-rmse:0.613053\n[481]\tvalidation_0-rmse:0.627594\tvalidation_1-rmse:0.61305\n[482]\tvalidation_0-rmse:0.627083\tvalidation_1-rmse:0.613041\n[483]\tvalidation_0-rmse:0.626908\tvalidation_1-rmse:0.613055\n[484]\tvalidation_0-rmse:0.626737\tvalidation_1-rmse:0.61311\n[485]\tvalidation_0-rmse:0.626684\tvalidation_1-rmse:0.61308\n[486]\tvalidation_0-rmse:0.62637\tvalidation_1-rmse:0.613063\n[487]\tvalidation_0-rmse:0.626274\tvalidation_1-rmse:0.61312\n[488]\tvalidation_0-rmse:0.626114\tvalidation_1-rmse:0.613183\n[489]\tvalidation_0-rmse:0.62562\tvalidation_1-rmse:0.613177\n[490]\tvalidation_0-rmse:0.625453\tvalidation_1-rmse:0.613203\n[491]\tvalidation_0-rmse:0.62515\tvalidation_1-rmse:0.613151\n[492]\tvalidation_0-rmse:0.625058\tvalidation_1-rmse:0.613211\n[493]\tvalidation_0-rmse:0.624969\tvalidation_1-rmse:0.613273\n[494]\tvalidation_0-rmse:0.624847\tvalidation_1-rmse:0.613307\n[495]\tvalidation_0-rmse:0.624684\tvalidation_1-rmse:0.613366\n[496]\tvalidation_0-rmse:0.624206\tvalidation_1-rmse:0.613385\n[497]\tvalidation_0-rmse:0.624171\tvalidation_1-rmse:0.613411\n[498]\tvalidation_0-rmse:0.624069\tvalidation_1-rmse:0.613415\n[499]\tvalidation_0-rmse:0.623884\tvalidation_1-rmse:0.613471\n[500]\tvalidation_0-rmse:0.623724\tvalidation_1-rmse:0.613535\n[501]\tvalidation_0-rmse:0.623262\tvalidation_1-rmse:0.613519\n[502]\tvalidation_0-rmse:0.622929\tvalidation_1-rmse:0.613528\n[503]\tvalidation_0-rmse:0.622704\tvalidation_1-rmse:0.613447\n[504]\tvalidation_0-rmse:0.622626\tvalidation_1-rmse:0.613453\n[505]\tvalidation_0-rmse:0.62247\tvalidation_1-rmse:0.613521\n[506]\tvalidation_0-rmse:0.622394\tvalidation_1-rmse:0.613527\n[507]\tvalidation_0-rmse:0.622071\tvalidation_1-rmse:0.613702\n[508]\tvalidation_0-rmse:0.621852\tvalidation_1-rmse:0.613624\n[509]\tvalidation_0-rmse:0.621689\tvalidation_1-rmse:0.613623\n[510]\tvalidation_0-rmse:0.621531\tvalidation_1-rmse:0.613674\n[511]\tvalidation_0-rmse:0.62124\tvalidation_1-rmse:0.613679\n[512]\tvalidation_0-rmse:0.62115\tvalidation_1-rmse:0.613667\n[513]\tvalidation_0-rmse:0.621061\tvalidation_1-rmse:0.613723\n[514]\tvalidation_0-rmse:0.620813\tvalidation_1-rmse:0.613705\n[515]\tvalidation_0-rmse:0.620536\tvalidation_1-rmse:0.613639\n[516]\tvalidation_0-rmse:0.62045\tvalidation_1-rmse:0.61367\n[517]\tvalidation_0-rmse:0.620418\tvalidation_1-rmse:0.613698\n[518]\tvalidation_0-rmse:0.620343\tvalidation_1-rmse:0.613704\n[519]\tvalidation_0-rmse:0.619996\tvalidation_1-rmse:0.613713\n[520]\tvalidation_0-rmse:0.619546\tvalidation_1-rmse:0.613739\n[521]\tvalidation_0-rmse:0.619449\tvalidation_1-rmse:0.613743\n[522]\tvalidation_0-rmse:0.619017\tvalidation_1-rmse:0.613736\n[523]\tvalidation_0-rmse:0.618837\tvalidation_1-rmse:0.613745\n[524]\tvalidation_0-rmse:0.618688\tvalidation_1-rmse:0.613822\n[525]\tvalidation_0-rmse:0.618374\tvalidation_1-rmse:0.613819\n[526]\tvalidation_0-rmse:0.618302\tvalidation_1-rmse:0.613825\n[527]\tvalidation_0-rmse:0.61807\tvalidation_1-rmse:0.613832\n[528]\tvalidation_0-rmse:0.617925\tvalidation_1-rmse:0.613913\nStopping. Best iteration:\n[478]\tvalidation_0-rmse:0.627953\tvalidation_1-rmse:0.612963\n\nWOODBTU_LOG score: 0.9503546841036774\n"}],"source":"from xgboost import XGBRegressor\n\nxgbr_models = {}\n\nxgbr_preprocessor = Pipeline([('OneHotEncoder', oneHot)])\nX_train_t = xgbr_preprocessor.fit_transform(X_train)\nX_val_t = xgbr_preprocessor.transform(X_val)\n\nfor i in range(len(potential_targets)):\n    target = potential_targets[i]\n    print('Training on', target)\n    y_train_t = y_train.iloc[:,i]\n    y_val_t = y_val.iloc[:,i]\n\n    xgbr_model = None\n    xgbr_model = XGBRegressor(  random_state=1,\n                                n_jobs=-2,\n                                n_estimators=4000,\n                                min_samples_leaf=35,\n                                # max_depth=9,\n                                learning_rate=.025,\n                                verbosity=0)\n\n    eval_set = [(X_train_t, y_train_t),\n                (X_val_t, y_val_t)]\n    xgbr_model.fit(X_train_t, y_train_t, eval_set=eval_set, early_stopping_rounds=50)\n\n    xgbr_models[target] = xgbr_model\n    print(target, 'score:', xgbr_models[target].score(X_val_t, y_val_t))\n\n\n\n# \n\n# multioutputregressor = MultiOutputRegressor(xgbr_model)\n# # multioutputregressor.set_params(eval_set=eval_set, early_stopping_rounds=50)\n# \n\n# # xgbr_model.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds=50)\n\n# # xgbr_model.score(X_val, y_val)"},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"TOTALBTU score: 0.6137520894217305\nTOTALDOL score: 0.5398324314709099\nKWH score: 0.5487202492864383\nBTUEL score: 0.5484183369934006\nDOLLAREL score: 0.47901158691385276\nCUFEETNG score: 0.7640836777388428\nBTUNG score: 0.765697741591308\nDOLLARNG score: 0.7838371338662781\nGALLONLP score: 0.8206387288250659\nBTULP score: 0.820635685908099\nDOLLARLP score: 0.8127246653392889\nGALLONFO score: 0.9053669244215432\nBTUFO score: 0.904555144611364\nDOLLARFO score: 0.8814024898652105\nWOODAMT score: 0.8891307430239677\nWOODBTU score: 0.7194427516849117\nTOTALBTU_LOG score: 0.6733350460697828\nTOTALDOL_LOG score: 0.5815517282065408\nKWH_LOG score: 0.58497215173913\nBTUEL_LOG score: 0.5855407391651758\nDOLLAREL_LOG score: 0.5099109193587947\nCUFEETNG_LOG score: 0.9627945363927972\nBTUNG_LOG score: 0.9629009102141136\nDOLLARNG_LOG score: 0.9594143258170718\nGALLONLP_LOG score: 0.9106651887977384\nBTULP_LOG score: 0.8694739877833287\nDOLLARLP_LOG score: 0.8861833539882978\nGALLONFO_LOG score: 0.9797290309180081\nBTUFO_LOG score: 0.9929966284586199\nDOLLARFO_LOG score: 0.9840071004212378\nWOODAMT_LOG score: 0.9790272885990483\nWOODBTU_LOG score: 0.9503546841036774\n"},{"data":{"text/plain":"['./Project-2/xgbr_models/preprocessor_pipeline.joblib']"},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":"# Save each model\nfor i in range(len(potential_targets)):\n    target = potential_targets[i]\n    y_val_t = y_val.iloc[:,i]\n    print(target, 'score:', xgbr_models[target].score(X_val_t, y_val_t))\n\n    joblib.dump(xgbr_models[target], './Project-2/xgbr_models/'+target+'_model.joblib', compress=True)\n\njoblib.dump(xgbr_preprocessor, './Project-2/xgbr_models/preprocessor_pipeline.joblib', compress=True)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}