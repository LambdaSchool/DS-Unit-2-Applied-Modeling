{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_applied_modeling_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nCc3XZEyG3XV"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 3, Module 1*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Define ML problems\n",
        "\n",
        "You will use your portfolio project dataset for all assignments this sprint.\n",
        "\n",
        "## Assignment\n",
        "\n",
        "Complete these tasks for your project, and document your decisions.\n",
        "\n",
        "- [X] Choose your target. Which column in your tabular dataset will you predict?\n",
        "- [X] Is your problem regression or classification?\n",
        "- [X] How is your target distributed?\n",
        "    - Classification: How many classes? Are the classes imbalanced?\n",
        "    - Regression: Is the target right-skewed? If so, you may want to log transform the target.\n",
        "- [X] Choose which observations you will use to train, validate, and test your model.\n",
        "    - Are some observations outliers? Will you exclude them?\n",
        "    - Will you do a random split or a time-based split?\n",
        "- [X] Choose your evaluation metric(s).\n",
        "    - Classification: Is your majority class frequency > 50% and < 70% ? If so, you can just use accuracy if you want. Outside that range, accuracy could be misleading. What evaluation metric will you choose, in addition to or instead of accuracy?\n",
        "- [X] Begin to clean and explore your data.\n",
        "- [ ] Begin to choose which features, if any, to exclude. Would some features \"leak\" future information?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plITiwb0sawh",
        "colab_type": "text"
      },
      "source": [
        "The target is MVP award result.\n",
        "\n",
        "The problem is classification. \n",
        "\n",
        "There are two classes and they are imbalanced. The two classes are currently 'won' and 'did not win.' I may change it to actual result position (1st, 2nd, 3rd, other).\n",
        "\n",
        "I will break down the data into decades. I plan to use a train-test split based on year for individual decades as well as a split for the dataset as a whole to find out which is more accurate. The dataset only includes non-pitchers, so observations for years where a pitcher won the award will not be used.\n",
        "\n",
        "Because the majority class frequency will be nearly 100%, I will use positive class precision and recall as evaluation metrics."
      ]
    }
  ]
}