{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCc3XZEyG3XV"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 3*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Permutation & Boosting\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Complete these tasks for your project, and document your work.\n",
    "\n",
    "- [ ] If you haven't completed assignment #1, please do so first.\n",
    "- [ ] Continue to clean and explore your data. Make exploratory visualizations.\n",
    "- [ ] Fit a model. Does it beat your baseline? \n",
    "- [ ] Try xgboost.\n",
    "- [ ] Get your model's permutation importances.\n",
    "\n",
    "You should try to complete an initial model today, because the rest of the week, we're making model interpretation visualizations.\n",
    "\n",
    "But, if you aren't ready to try xgboost and permutation importances with your dataset today, that's okay. You can practice with another dataset instead. You may choose any dataset you've worked with previously.\n",
    "\n",
    "The data subdirectory includes the Titanic dataset for classification and the NYC apartments dataset for regression. You may want to choose one of these datasets, because example solutions will be available for each.\n",
    "\n",
    "\n",
    "## Reading\n",
    "\n",
    "Top recommendations in _**bold italic:**_\n",
    "\n",
    "#### Permutation Importances\n",
    "- _**[Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)**_\n",
    "- [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "\n",
    "#### (Default) Feature Importances\n",
    "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "\n",
    "#### Gradient Boosting\n",
    "  - [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "  - [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf), Chapter 8\n",
    "  - _**[Gradient Boosting Explained](https://www.gormanalysis.com/blog/gradient-boosting-explained/)**_ — Ben Gorman\n",
    "  - [Gradient Boosting Explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html) — Alex Rogozhnikov\n",
    "  - [How to explain gradient boosting](https://explained.ai/gradient-boosting/) — Terence Parr & Jeremy Howard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas_profiling\n",
    "\n",
    "from functions import wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "__location__ = Path(os.getcwd()).parent\n",
    "__data_dir__ = __location__ / \"data\"\n",
    "\n",
    "df = pd.read_csv(str(__data_dir__) + '/anime.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = wrangle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split X and y\n",
    "### Consider having multiple y's, to do this must change wrangle from dropping certain columns, check phone notepad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clean['rank'].copy()\n",
    "X = clean.drop('rank',axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pipe = make_pipeline(OneHotEncoder(use_cat_names=True),XGBRegressor(n_jobs=-1,\n",
    "                                                                      random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB train : 0.7595123869386505\n",
      "XGB test: 0.5994204568174826\n",
      "\n",
      "Base MAE: 3210.093119097792\n",
      "\n",
      "Train MAE: 1366.8743007766166\n",
      "Test MAE: 1787.1004934340785\n"
     ]
    }
   ],
   "source": [
    "baseline = [y_train.mean()]\n",
    "\n",
    "print(f\"XGB train : {xg_pipe.score(X_train,y_train)}\")\n",
    "print(f\"XGB test: {xg_pipe.score(X_test,y_test)}\")\n",
    "print()\n",
    "print(f\"Base MAE: {mean_absolute_error(y_train,baseline*len(y_train))}\\n\")\n",
    "print(f\"Train MAE: {mean_absolute_error(y_train,xg_pipe.predict(X_train))}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(y_test,xg_pipe.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(OneHotEncoder(use_cat_names=True),\n",
    "                    XGBRegressor(n_jobs=-1,random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'xgbregressor__learning_rate': [.01,.02,.03,.04,.05,.06,.07,.08,.09,.1,],\n",
    "    'xgbregressor__max_delta_step': [0,1,2,3,4,5,6,7,8,9,10],\n",
    "    'xgbregressor__booster': ['gbtree','gblinear','dart'],\n",
    "    'xgbregressor__max_depth': [5,10,30,50,None],\n",
    "    'xgbregressor__min_child_weight': [1,2,3,4,5,10,15]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_search = RandomizedSearchCV(\n",
    "    estimator = pipe,\n",
    "    param_distributions = params,\n",
    "    n_iter=500,\n",
    "    verbose=1,\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=8)]: Done 1784 tasks      | elapsed: 38.5min\n",
      "[Parallel(n_jobs=8)]: Done 2434 tasks      | elapsed: 49.7min\n",
      "[Parallel(n_jobs=8)]: Done 2500 out of 2500 | elapsed: 51.1min finished\n"
     ]
    }
   ],
   "source": [
    "#tune_search.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgbregressor__min_child_weight': 10,\n",
       " 'xgbregressor__max_depth': 10,\n",
       " 'xgbregressor__max_delta_step': 0,\n",
       " 'xgbregressor__learning_rate': 0.02,\n",
       " 'xgbregressor__booster': 'dart'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tune_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(OneHotEncoder(use_cat_names=True),\n",
    "                    XGBRegressor(n_jobs=-1,random_state=42,\n",
    "                                n_estimators=500,min_child_weight=3,\n",
    "                                max_depth=None,learning_rate=0.1,\n",
    "                                booster='gbtree',max_delta_step=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest train : 0.7854710621391039\n",
      "Forest test: 0.611687962113775\n",
      "\n",
      "Base MAE: 3210.093119097792\n",
      "\n",
      "Train MAE: 1279.1128902135588\n",
      "Test MAE: 1756.676769762928\n"
     ]
    }
   ],
   "source": [
    "baseline = [y_train.mean()]\n",
    "\n",
    "print(f\"Forest train : {pipe.score(X_train,y_train)}\")\n",
    "print(f\"Forest test: {pipe.score(X_test,y_test)}\")\n",
    "print()\n",
    "print(f\"Base MAE: {mean_absolute_error(y_train,baseline*len(y_train))}\\n\")\n",
    "print(f\"Train MAE: {mean_absolute_error(y_train,pipe.predict(X_train))}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(y_test,pipe.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(OneHotEncoder(use_cat_names=True),\n",
    "                    XGBRegressor(n_jobs=-1,random_state=42,\n",
    "                                n_estimators=500,min_child_weight=10,\n",
    "                                max_depth=10,learning_rate=0.02,\n",
    "                                booster='dart',max_delta_step=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest train : 0.7615501538682983\n",
      "Forest test: 0.6151730665688578\n",
      "\n",
      "Base MAE: 3210.093119097792\n",
      "\n",
      "Train MAE: 1373.4839870986207\n",
      "Test MAE: 1755.5248652389485\n"
     ]
    }
   ],
   "source": [
    "baseline = [y_train.mean()]\n",
    "\n",
    "print(f\"Forest train : {pipe.score(X_train,y_train)}\")\n",
    "print(f\"Forest test: {pipe.score(X_test,y_test)}\")\n",
    "print()\n",
    "print(f\"Base MAE: {mean_absolute_error(y_train,baseline*len(y_train))}\\n\")\n",
    "print(f\"Train MAE: {mean_absolute_error(y_train,pipe.predict(X_train))}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(y_test,pipe.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(pipe, X_test, y_test, \n",
    "                                n_repeats=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'feature': X_test.columns,\n",
    "                   'importances_mean': np.round(result['importances_mean'], 3),\n",
    "                   'importances_std': result['importances_std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importances_mean</th>\n",
       "      <th>importances_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_related</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.013685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rating</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.003504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>source</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.003138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>duration</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.007050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>studio</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.004501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.005465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>episodes</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.005903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hentai/romance</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.003266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>drama</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.001964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fantasy/sci-fi</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.002360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>comedy</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.001122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>youth</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adventure/psychological</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>military</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>misc</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sports</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airing</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>status</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    feature  importances_mean  importances_std\n",
       "8               num_related             0.222         0.013685\n",
       "6                    rating             0.150         0.003504\n",
       "1                    source             0.117         0.003138\n",
       "5                  duration             0.095         0.007050\n",
       "7                    studio             0.048         0.004501\n",
       "0                      type             0.039         0.005465\n",
       "2                  episodes             0.027         0.005903\n",
       "9            hentai/romance             0.025         0.003266\n",
       "13                    drama             0.019         0.001964\n",
       "14           fantasy/sci-fi             0.013         0.002360\n",
       "15                   comedy             0.013         0.001122\n",
       "11                    youth             0.009         0.002658\n",
       "12  adventure/psychological             0.008         0.001210\n",
       "16                 military             0.004         0.001816\n",
       "17                     misc             0.002         0.000442\n",
       "10                   sports             0.001         0.000929\n",
       "4                    airing             0.000         0.000000\n",
       "18                  Unknown            -0.000         0.000252\n",
       "3                    status            -0.001         0.000497"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='importances_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
