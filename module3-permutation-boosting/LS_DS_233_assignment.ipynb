{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCc3XZEyG3XV"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 3*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Permutation & Boosting\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Complete these tasks for your project, and document your work.\n",
    "\n",
    "- [ ] If you haven't completed assignment #1, please do so first.\n",
    "- [ ] Continue to clean and explore your data. Make exploratory visualizations.\n",
    "- [ ] Fit a model. Does it beat your baseline? \n",
    "- [ ] Try xgboost.\n",
    "- [ ] Get your model's permutation importances.\n",
    "\n",
    "You should try to complete an initial model today, because the rest of the week, we're making model interpretation visualizations.\n",
    "\n",
    "But, if you aren't ready to try xgboost and permutation importances with your dataset today, that's okay. You can practice with another dataset instead. You may choose any dataset you've worked with previously.\n",
    "\n",
    "The data subdirectory includes the Titanic dataset for classification and the NYC apartments dataset for regression. You may want to choose one of these datasets, because example solutions will be available for each.\n",
    "\n",
    "\n",
    "## Reading\n",
    "\n",
    "Top recommendations in _**bold italic:**_\n",
    "\n",
    "#### Permutation Importances\n",
    "- _**[Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)**_\n",
    "- [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "\n",
    "#### (Default) Feature Importances\n",
    "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "\n",
    "#### Gradient Boosting\n",
    "  - [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "  - _**[A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)**_\n",
    "  - [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) Chapter 8\n",
    "  - [Gradient Boosting Explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)\n",
    "  - _**[Boosting](https://www.youtube.com/watch?v=GM3CDQfQ4sw) (2.5 minute video)**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued from assignment 232\n",
    "## Why is my model junky?\n",
    "Hypothesis: My model has low precision and high accuracy because of the class imbalance (0 = 99.4%, 1 = 0.6%), and makes bad predictions because the training on 1,000 observations is not meaningful\n",
    "\n",
    "## Test strategy\n",
    "### Test 1 more data\n",
    "Simply just feed in more data\n",
    "### Test 2 random sample more data\n",
    "Same thing, but lets sample instead of read in sequentially\n",
    "### Test 3 stratify sample more data\n",
    "Same thing, but lets try stratification for sampling\n",
    "\n",
    "## Next steps\n",
    "Once I have a reasonable data sampling strategy I can try permuting and an XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "Get data iterable, create basic pipeline, create data sets and test one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import pandas as pd\n",
    "\n",
    "folder = '../../DS-Unit-2-Build/bosch-production-line-performance/'\n",
    "\n",
    "num_iter = pd.read_csv(folder + 'train_numeric.csv', iterator = True, chunksize = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: More data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 970)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>L0_S0_F0</th>\n",
       "      <th>L0_S0_F2</th>\n",
       "      <th>L0_S0_F4</th>\n",
       "      <th>L0_S0_F6</th>\n",
       "      <th>L0_S0_F8</th>\n",
       "      <th>L0_S0_F10</th>\n",
       "      <th>L0_S0_F12</th>\n",
       "      <th>L0_S0_F14</th>\n",
       "      <th>L0_S0_F16</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_S50_F4245</th>\n",
       "      <th>L3_S50_F4247</th>\n",
       "      <th>L3_S50_F4249</th>\n",
       "      <th>L3_S50_F4251</th>\n",
       "      <th>L3_S50_F4253</th>\n",
       "      <th>L3_S51_F4256</th>\n",
       "      <th>L3_S51_F4258</th>\n",
       "      <th>L3_S51_F4260</th>\n",
       "      <th>L3_S51_F4262</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 970 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  L0_S0_F0  L0_S0_F2  L0_S0_F4  L0_S0_F6  L0_S0_F8  L0_S0_F10  L0_S0_F12  \\\n",
       "0   4     0.030    -0.034    -0.197    -0.179     0.118      0.116     -0.015   \n",
       "1   6       NaN       NaN       NaN       NaN       NaN        NaN        NaN   \n",
       "2   7     0.088     0.086     0.003    -0.052     0.161      0.025     -0.015   \n",
       "3   9    -0.036    -0.064     0.294     0.330     0.074      0.161      0.022   \n",
       "4  11    -0.055    -0.086     0.294     0.330     0.118      0.025      0.030   \n",
       "\n",
       "   L0_S0_F14  L0_S0_F16  ...  L3_S50_F4245  L3_S50_F4247  L3_S50_F4249  \\\n",
       "0     -0.032      0.020  ...           NaN           NaN           NaN   \n",
       "1        NaN        NaN  ...           NaN           NaN           NaN   \n",
       "2     -0.072     -0.225  ...           NaN           NaN           NaN   \n",
       "3      0.128     -0.026  ...           NaN           NaN           NaN   \n",
       "4      0.168     -0.169  ...           NaN           NaN           NaN   \n",
       "\n",
       "   L3_S50_F4251  L3_S50_F4253  L3_S51_F4256  L3_S51_F4258  L3_S51_F4260  \\\n",
       "0           NaN           NaN           NaN           NaN           NaN   \n",
       "1           NaN           NaN           NaN           NaN           NaN   \n",
       "2           NaN           NaN           NaN           NaN           NaN   \n",
       "3           NaN           NaN           NaN           NaN           NaN   \n",
       "4           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "   L3_S51_F4262  Response  \n",
       "0           NaN         0  \n",
       "1           NaN         0  \n",
       "2           NaN         0  \n",
       "3           NaN         0  \n",
       "4           NaN         0  \n",
       "\n",
       "[5 rows x 970 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#time to chonk it in.  I'm going to kinda play this by ear, but I think I need to increase\n",
    "#my data by a factor of 10.  Interesting Dash idea! make a few graphs that show the importance\n",
    "#of data size, sampling methods, models etc.  Store that away for later\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for i in range(10):\n",
    "    chunks.append(num_iter.get_chunk())\n",
    "    \n",
    "big_samp = pd.concat(chunks)\n",
    "print(big_samp.shape)\n",
    "big_samp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
