{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "LS_DS_233.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U2ha9OWxf0jw"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 3, Module 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-hTictxWYih7"
      },
      "source": [
        "# Permutation & Boosting\n",
        "\n",
        "- Get **permutation importances** for model interpretation and feature selection\n",
        "- Use xgboost for **gradient boosting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wMejJg0w8v76"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Run the code cell below. You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab.\n",
        "\n",
        "Libraries:\n",
        "\n",
        "- category_encoders\n",
        "- [**eli5**](https://eli5.readthedocs.io/en/latest/)\n",
        "- matplotlib\n",
        "- numpy\n",
        "- pandas\n",
        "- scikit-learn\n",
        "- [**xgboost**](https://xgboost.readthedocs.io/en/latest/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BFQMky3CYih-",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "    !pip install eli5\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d797ELQHbHWZ",
        "colab_type": "text"
      },
      "source": [
        "We'll go back to Tanzania Waterpumps for this lesson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z-TExplb_Slf",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Merge train_features.csv & train_labels.csv\n",
        "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
        "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
        "\n",
        "# Read test_features.csv & sample_submission.csv\n",
        "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
        "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
        "\n",
        "\n",
        "# Split train into train & val\n",
        "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
        "                              stratify=train['status_group'], random_state=42)\n",
        "\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
        "    \n",
        "    # Prevent SettingWithCopyWarning\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    # Also create a \"missing indicator\" column, because the fact that\n",
        "    # values are missing may be a predictive signal.\n",
        "    cols_with_zeros = ['longitude', 'latitude', 'construction_year', \n",
        "                       'gps_height', 'population']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "        X[col+'_MISSING'] = X[col].isnull()\n",
        "            \n",
        "    # Drop duplicate columns\n",
        "    duplicates = ['quantity_group', 'payment_type']\n",
        "    X = X.drop(columns=duplicates)\n",
        "    \n",
        "    # Drop recorded_by (never varies) and id (always varies, random)\n",
        "    unusable_variance = ['recorded_by', 'id']\n",
        "    X = X.drop(columns=unusable_variance)\n",
        "    \n",
        "    # Convert date_recorded to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "    \n",
        "    # Extract components from date_recorded, then drop the original column\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    X['month_recorded'] = X['date_recorded'].dt.month\n",
        "    X['day_recorded'] = X['date_recorded'].dt.day\n",
        "    X = X.drop(columns='date_recorded')\n",
        "    \n",
        "    # Engineer feature: how many years from construction_year to date_recorded\n",
        "    X['years'] = X['year_recorded'] - X['construction_year']\n",
        "    X['years_MISSING'] = X['years'].isnull()\n",
        "    \n",
        "    # return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rhg8PQKt_jzP",
        "colab": {}
      },
      "source": [
        "# Arrange data into X features matrix and y target vector\n",
        "target = 'status_group'\n",
        "X_train = train.drop(columns=target)\n",
        "y_train = train[target]\n",
        "X_val = val.drop(columns=target)\n",
        "y_val = val[target]\n",
        "X_test = test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8lB4z5l_eml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b58de06-6aed-4bdb-ec8f-4f3fdcd489d4"
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='median'), \n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 0.8135521885521886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kseet_0ebHWk",
        "colab_type": "text"
      },
      "source": [
        "# Get permutation importances for model interpretation and feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMuA4GPCbHWl",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrCtMPr4bHWm",
        "colab_type": "text"
      },
      "source": [
        "Default Feature Importances are fast, but Permutation Importances may be more accurate.\n",
        "\n",
        "These links go deeper with explanations and examples:\n",
        "\n",
        "- Permutation Importances\n",
        "  - [Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)\n",
        "  - [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
        "- (Default) Feature Importances\n",
        "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
        "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7HOayKBOYiit"
      },
      "source": [
        "There are three types of feature importances:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4bRhsxENYiiu"
      },
      "source": [
        "### 1. (Default) Feature Importances\n",
        "\n",
        "Fastest, good for first estimates, but be aware:\n",
        "\n",
        "\n",
        "\n",
        ">**When the dataset has two (or more) correlated features, then from the point of view of the model, any of these correlated features can be used as the predictor, with no concrete preference of one over the others.** But once one of them is used, the importance of others is significantly reduced since effectively the impurity they can remove is already removed by the first feature. As a consequence, they will have a lower reported importance. This is not an issue when we want to use feature selection to reduce overfitting, since it makes sense to remove features that are mostly duplicated by other features. But when interpreting the data, it can lead to the incorrect conclusion that one of the variables is a strong predictor while the others in the same group are unimportant, while actually they are very close in terms of their relationship with the response variable. — [Selecting good features – Part III: random forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/) \n",
        "\n",
        "\n",
        " \n",
        " > **The scikit-learn Random Forest feature importance ... tends to inflate the importance of continuous or high-cardinality categorical variables.** ... Breiman and Cutler, the inventors of Random Forests, indicate that this method of “adding up the gini decreases for each individual variable over all trees in the forest gives a **fast** variable importance that is often very consistent with the permutation importance measure.” —  [Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BNVm6f7mYiiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "4de5c6f9-6dde-46a8-a8f7-2fcfa46bc13d"
      },
      "source": [
        "# Get feature importances\n",
        "rf = pipeline.named_steps['randomforestclassifier']\n",
        "importances = pd.Series(rf.feature_importances_, X_train.columns)\n",
        "\n",
        "# Plot feature importances\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 20\n",
        "plt.figure(figsize=(10,n/2))\n",
        "plt.title(f'Top {n} features')\n",
        "importances.sort_values()[-n:].plot.barh(color='grey');"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArYAAAJOCAYAAABCwkSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSeoEC1tJkiR1goWtJEmSOuH/A+gFpoboqVw/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y8HzLcCBYiiv"
      },
      "source": [
        "### 2. Drop-Column Importance\n",
        "\n",
        "The best in theory, but too slow in practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DQAOlERnYiiw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f3bc22a9-ca61-496a-ea2a-6e1651710f02"
      },
      "source": [
        "column  = 'quantity'\n",
        "\n",
        "# Fit without column\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='median'), \n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        ")\n",
        "pipeline.fit(X_train.drop(columns=column), y_train)\n",
        "score_without = pipeline.score(X_val.drop(columns=column), y_val)\n",
        "print(f'Validation Accuracy without {column}: {score_without}')\n",
        "\n",
        "# Fit with column\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='median'), \n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        ")\n",
        "pipeline.fit(X_train, y_train)\n",
        "score_with = pipeline.score(X_val, y_val)\n",
        "print(f'Validation Accuracy with {column}: {score_with}')\n",
        "\n",
        "# Compare the error with & without column\n",
        "print(f'Drop-Column Importance for {column}: {score_with - score_without}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy without quantity: 0.7771043771043771\n",
            "Validation Accuracy with quantity: 0.8135521885521886\n",
            "Drop-Column Importance for quantity: 0.03644781144781151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Vu39wGkYiix"
      },
      "source": [
        "### 3. Permutation Importance\n",
        "\n",
        "Permutation Importance is a good compromise between Feature Importance based on impurity reduction (which is the fastest) and Drop Column Importance (which is the \"best.\")\n",
        "\n",
        "[The ELI5 library documentation explains,](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)\n",
        "\n",
        "> Importance can be measured by looking at how much the score (accuracy, F1, R^2, etc. - any score we’re interested in) decreases when a feature is not available.\n",
        ">\n",
        "> To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. ...\n",
        ">\n",
        ">To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the same distribution as original feature values (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed.\n",
        ">\n",
        ">The method is most suitable for computing feature importances when a number of columns (features) is not huge; it can be resource-intensive otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GYCiEx7zYiiy"
      },
      "source": [
        "### Do-It-Yourself way, for intuition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TksOf_n2Yiiy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5bea001b-6822-45de-fec1-417ad79aafcf"
      },
      "source": [
        "feature = 'quantity'\n",
        "X_val[feature].head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3290     insufficient\n",
              "47666    insufficient\n",
              "2538           enough\n",
              "53117          enough\n",
              "51817          enough\n",
              "Name: quantity, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q23nz6V7kHeF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "45fa4d7a-3948-461e-8fbe-a8dd4914ec35"
      },
      "source": [
        "X_val[feature].value_counts(normalize=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "enough          0.557155\n",
              "insufficient    0.250505\n",
              "dry             0.111532\n",
              "seasonal        0.067845\n",
              "unknown         0.012963\n",
              "Name: quantity, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_blxoEl6kJ4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val_permuted = X_val.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzYC67dxkWSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ec936db9-d698-4144-b560-f3dc017939ad"
      },
      "source": [
        "X_val_permuted[feature] = np.random.permutation(X_val[feature])\n",
        "X_val_permuted[feature].value_counts(normalize=True)\n",
        "# we now have randomized/shuffled values but the same distriubtion "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "enough          0.557155\n",
              "insufficient    0.250505\n",
              "dry             0.111532\n",
              "seasonal        0.067845\n",
              "unknown         0.012963\n",
              "Name: quantity, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv1QIX6Dkvaw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9d83eb44-e74a-4f84-b846-c08df21c4afe"
      },
      "source": [
        "score_permuted = pipeline.score(X_val_permuted, y_val)\n",
        "\n",
        "print('Validaiton Accuracy with feature permutation:',feature, ':', score_permuted)\n",
        "print('Validaiton Accuracy with:',feature, ':', score_with)\n",
        "print('Validaiton Accuracy difference:', (score_with - score_permuted))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validaiton Accuracy with feature permutation: quantity : 0.7071548821548822\n",
            "Validaiton Accuracy with: quantity : 0.8135521885521886\n",
            "Validaiton Accuracy difference: 0.10639730639730638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCDT7FwEk-7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0LYk19SNYii7"
      },
      "source": [
        "### With eli5 library\n",
        "\n",
        "For more documentation on using this library, see:\n",
        "- [eli5.sklearn.PermutationImportance](https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#eli5.sklearn.permutation_importance.PermutationImportance)\n",
        "- [eli5.show_weights](https://eli5.readthedocs.io/en/latest/autodocs/eli5.html#eli5.show_weights)\n",
        "- [scikit-learn user guide, `scoring` parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)\n",
        "\n",
        "eli5 doesn't work with pipelines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erdxcBtqkUOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hpSemTkFFP8i",
        "colab": {}
      },
      "source": [
        "# Ignore warnings\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q07yW9k-Yii8"
      },
      "source": [
        "### We can use importances for feature selection\n",
        "\n",
        "For example, we can remove features with zero importance. The model trains faster and the score does not decrease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tZrPFyEMYii9",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fl67bCR7WY6j"
      },
      "source": [
        "# Use xgboost for gradient boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyIF2OQLbHW9",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OYPMMdjbHW-",
        "colab_type": "text"
      },
      "source": [
        "In the Random Forest lesson, you learned this advice:\n",
        "\n",
        "#### Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
        "- \"Tree Ensembles\" means Random Forest or **Gradient Boosting** models. \n",
        "- [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) with labeled, tabular data.\n",
        "- Why? Because trees can fit non-linear, non-[monotonic](https://en.wikipedia.org/wiki/Monotonic_function) relationships, and [interactions](https://christophm.github.io/interpretable-ml-book/interaction.html) between features.\n",
        "- A single decision tree, grown to unlimited depth, will [overfit](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). We solve this problem by ensembling trees, with bagging (Random Forest) or **[boosting](https://www.youtube.com/watch?v=GM3CDQfQ4sw)** (Gradient Boosting).\n",
        "- Random Forest's advantage: may be less sensitive to hyperparameters. **Gradient Boosting's advantage:** may get better predictive accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyxJoPakbHW_",
        "colab_type": "text"
      },
      "source": [
        "Like Random Forest, Gradient Boosting uses ensembles of trees. But the details of the ensembling technique are different:\n",
        "\n",
        "### Understand the difference between boosting & bagging\n",
        "\n",
        "Boosting (used by Gradient Boosting) is different than Bagging (used by Random Forests). \n",
        "\n",
        "Here's an excerpt from [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) Chapter 8.2.3, Boosting:\n",
        "\n",
        ">Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.\n",
        ">\n",
        ">**Boosting works in a similar way, except that the trees are grown _sequentially_: each tree is grown using information from previously grown trees.**\n",
        ">\n",
        ">Unlike fitting a single large decision tree to the data, which amounts to _fitting the data hard_ and potentially overfitting, the boosting approach instead _learns slowly._ Given the current model, we fit a decision tree to the residuals from the model.\n",
        ">\n",
        ">We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes. **By fitting small trees to the residuals, we slowly improve fˆ in areas where it does not perform well.**\n",
        ">\n",
        ">Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.\n",
        "\n",
        "This high-level overview is all you need to know for now. If you want to go deeper, we recommend you watch the StatQuest videos on gradient boosting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct3sWL-PbHXA",
        "colab_type": "text"
      },
      "source": [
        "Let's write some code. We have lots of options for which libraries to use:\n",
        "\n",
        "#### Python libraries for Gradient Boosting\n",
        "- [scikit-learn Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) — slower than other libraries, but [the new version may be better](https://twitter.com/amuellerml/status/1129443826945396737)\n",
        "  - Anaconda: already installed\n",
        "  - Google Colab: already installed\n",
        "- [xgboost](https://xgboost.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://xiaoxiaowang87.github.io/monotonicity_constraint/)\n",
        "  - Anaconda, Mac/Linux: `conda install -c conda-forge xgboost`\n",
        "  - Windows: `conda install -c anaconda py-xgboost`\n",
        "  - Google Colab: already installed\n",
        "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://blog.datadive.net/monotonicity-constraints-in-machine-learning/)\n",
        "  - Anaconda: `conda install -c conda-forge lightgbm`\n",
        "  - Google Colab: already installed\n",
        "- [CatBoost](https://catboost.ai/) — can accept missing values and use [categorical features](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html) without preprocessing\n",
        "  - Anaconda: `conda install -c conda-forge catboost`\n",
        "  - Google Colab: `pip install catboost`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaJmHP78bHXB",
        "colab_type": "text"
      },
      "source": [
        "In this lesson, you'll use a new library, xgboost — But it has an API that's almost the same as scikit-learn, so it won't be a hard adjustment!\n",
        "\n",
        "#### [XGBoost Python API Reference: Scikit-Learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wsnJRKjfWYph",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eCjVSlD_XJr2"
      },
      "source": [
        "#### [Avoid Overfitting By Early Stopping With XGBoost In Python](https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/)\n",
        "\n",
        "Why is early stopping better than a For loop, or GridSearchCV, to optimize `n_estimators`?\n",
        "\n",
        "With early stopping, if `n_iterations` is our number of iterations, then we fit `n_iterations` decision trees.\n",
        "\n",
        "With a for loop, or GridSearchCV, we'd fit `sum(range(1,n_rounds+1))` trees.\n",
        "\n",
        "But it doesn't work well with pipelines. You may need to re-run multiple times with different values of other parameters such as `max_depth` and `learning_rate`.\n",
        "\n",
        "#### XGBoost parameters\n",
        "- [Notes on parameter tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html)\n",
        "- [Parameters documentation](https://xgboost.readthedocs.io/en/latest/parameter.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZNX3IKftXBFS",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZF7-ml6BhRRf"
      },
      "source": [
        "### Try adjusting these hyperparameters\n",
        "\n",
        "#### Random Forest\n",
        "- class_weight (for imbalanced classes)\n",
        "- max_depth (usually high, can try decreasing)\n",
        "- n_estimators (too low underfits, too high wastes time)\n",
        "- min_samples_leaf (increase if overfitting)\n",
        "- max_features (decrease for more diverse trees)\n",
        "\n",
        "#### Xgboost\n",
        "- scale_pos_weight (for imbalanced classes)\n",
        "- max_depth (usually low, can try increasing)\n",
        "- n_estimators (too low underfits, too high wastes time/overfits) — Use Early Stopping!\n",
        "- learning_rate (too low underfits, too high overfits)\n",
        "\n",
        "For more ideas, see [Notes on Parameter Tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html) and [DART booster](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikZTOzs4bHXI",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will use your portfolio project dataset for all assignments this sprint. Complete these tasks for your project, and document your work.\n",
        "\n",
        "- Continue to clean and explore your data. Make exploratory visualizations.\n",
        "- Fit a model. Does it beat your baseline?\n",
        "- Try xgboost.\n",
        "- Get your model's permutation importances.\n",
        "\n",
        "You should try to complete an initial model today, because the rest of the week, we're making model interpretation visualizations.\n",
        "\n",
        "But, if you aren't ready to try xgboost and permutation importances with your dataset today, you can practice with another dataset instead. You may choose any dataset you've worked with previously."
      ]
    }
  ]
}